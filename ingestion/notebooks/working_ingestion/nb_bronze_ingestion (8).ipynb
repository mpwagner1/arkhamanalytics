{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20c0dca6-5393-4ea1-8f9b-109eba9ec744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, lit, current_timestamp, trim\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfcc3075-9985-4e3a-929e-89c028d24bcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration with domain folder structure\n",
    "CONTAINER = \"mg-gold-raw-files\"\n",
    "\n",
    "# Domain-specific folder mapping\n",
    "DOMAIN_FOLDER_MAP = {\n",
    "    \"Customer\": \"raw_customer\",\n",
    "    \"Carrier\": \"raw_carrier\",\n",
    "    \"Load Detail\": \"raw_load_details\"\n",
    "}\n",
    "\n",
    "# Processing subfolders within each domain folder\n",
    "PROCESSING_SUBFOLDERS = {\n",
    "    \"incoming\": \"incoming\",\n",
    "    \"processed\": \"processed\",\n",
    "    \"failed\": \"failed\"\n",
    "}\n",
    "\n",
    "# Legacy config (kept for backward compatibility)\n",
    "DOMAIN_FOLDER = \"raw_load_details\"\n",
    "INCOMING_FOLDER = \"incoming\"\n",
    "PROCESSED_FOLDER = \"processed\"\n",
    "\n",
    "BRONZE_SCHEMA = \"load_detail\"\n",
    "BRONZE_TABLE = \"load_transactions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "563a77ea-dabc-4824-8a73-2d6444c24b82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Bronze table configuration by domain\n",
    "BRONZE_TABLE_CONFIG = {\n",
    "    \"Customer\": {\n",
    "        \"schema\": \"customer\",\n",
    "        \"table\": \"customer_master\",\n",
    "        \"description\": \"Customer master data from MercuryGate Gold\"\n",
    "    },\n",
    "    \"Carrier\": {\n",
    "        \"schema\": \"carrier\",\n",
    "        \"table\": \"carrier_master\",\n",
    "        \"description\": \"Carrier master data from MercuryGate Gold\"\n",
    "    },\n",
    "    \"Load Detail\": {\n",
    "        \"schema\": \"load_detail\",\n",
    "        \"table\": \"load_transactions\",\n",
    "        \"description\": \"Load/shipment transaction details from MercuryGate Gold\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_bronze_config(domain: str) -> dict:\n",
    "    \"\"\"Get bronze table config for domain\"\"\"\n",
    "    if domain not in BRONZE_TABLE_CONFIG:\n",
    "        raise ValueError(f\"Unknown domain: {domain}\")\n",
    "    return BRONZE_TABLE_CONFIG[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d109d223-ff27-48d7-a3e4-c8e852402b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WidgetManager - Dynamic widget creation based on domain\n",
    "\n",
    "class WidgetManager:\n",
    "    \"\"\"Manages Databricks widgets with domain-specific behavior\"\"\"\n",
    "    \n",
    "    # Widget definitions\n",
    "    WIDGET_DEFINITIONS = {\n",
    "        \"processing_option\": {\n",
    "            \"type\": \"dropdown\",\n",
    "            \"default\": \"Load Detail\",\n",
    "            \"choices\": [\"Customer\", \"Carrier\", \"Load Detail\"],\n",
    "            \"label\": \"Data Domain\"\n",
    "        },\n",
    "        \"file_pattern\": {\n",
    "            \"type\": \"text\",\n",
    "            \"default\": \"\",\n",
    "            \"label\": \"File Pattern (optional)\"\n",
    "        },\n",
    "        \"customer_id\": {\n",
    "            \"type\": \"text\",\n",
    "            \"default\": \"\",\n",
    "            \"label\": \"Customer ID (optional)\",\n",
    "            \"domains\": [\"Customer\", \"Load Detail\"]\n",
    "        },\n",
    "        \"carrier_id\": {\n",
    "            \"type\": \"text\",\n",
    "            \"default\": \"\",\n",
    "            \"label\": \"Carrier ID (optional)\",\n",
    "            \"domains\": [\"Carrier\", \"Load Detail\"]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_widgets(domain: str = None):\n",
    "        \"\"\"Create widgets, optionally filtered by domain\"\"\"\n",
    "        print(\"Creating widgets...\")\n",
    "        \n",
    "        for widget_name, config in WidgetManager.WIDGET_DEFINITIONS.items():\n",
    "            # Skip domain-specific widgets if not applicable\n",
    "            if \"domains\" in config and domain and domain not in config[\"domains\"]:\n",
    "                continue\n",
    "            \n",
    "            # Create widget based on type\n",
    "            if config[\"type\"] == \"dropdown\":\n",
    "                dbutils.widgets.dropdown(\n",
    "                    widget_name,\n",
    "                    config[\"default\"],\n",
    "                    config[\"choices\"],\n",
    "                    config[\"label\"]\n",
    "                )\n",
    "            elif config[\"type\"] == \"text\":\n",
    "                dbutils.widgets.text(\n",
    "                    widget_name,\n",
    "                    config[\"default\"],\n",
    "                    config[\"label\"]\n",
    "                )\n",
    "        \n",
    "        print(f\" Widgets created for domain: {domain if domain else 'All'}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_all_widgets():\n",
    "        \"\"\"Remove all widgets\"\"\"\n",
    "        try:\n",
    "            dbutils.widgets.removeAll()\n",
    "            print(\" All widgets removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not remove widgets: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_widget_values() -> dict:\n",
    "        \"\"\"Get all current widget values\"\"\"\n",
    "        values = {}\n",
    "        for widget_name in WidgetManager.WIDGET_DEFINITIONS.keys():\n",
    "            try:\n",
    "                values[widget_name] = dbutils.widgets.get(widget_name)\n",
    "            except:\n",
    "                values[widget_name] = None\n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ede457d-9ef6-431c-8249-34a36d3d3489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating widgets...\n Widgets created for domain: All\n"
     ]
    }
   ],
   "source": [
    "# Create widgets using WidgetManager\n",
    "WidgetManager.create_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10664a0-528a-42a2-b437-6009a5a6808b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simple file processing function with domain folder support\n",
    "def process_csv_file(file_name: str, domain: str) -> DataFrame:\n",
    "    \"\"\"Process a single CSV file from domain-specific folder\"\"\"\n",
    "    \n",
    "    # Get domain folder\n",
    "    if domain in DOMAIN_FOLDER_MAP:\n",
    "        domain_folder = DOMAIN_FOLDER_MAP[domain]\n",
    "        file_path = f\"/mnt/{CONTAINER}/{domain_folder}/{PROCESSING_SUBFOLDERS['incoming']}/{file_name}\"\n",
    "    else:\n",
    "        # Fallback to legacy path\n",
    "        file_path = f\"/mnt/{CONTAINER}/{DOMAIN_FOLDER}/{INCOMING_FOLDER}/{file_name}\"\n",
    "    \n",
    "    print(f\"Reading: {file_name}\")\n",
    "    print(f\"  Path: {file_path}\")\n",
    "    \n",
    "    # Detect file properties\n",
    "    dbfs_path = f\"/dbfs{file_path}\"\n",
    "    props = detect_file_properties(dbfs_path)\n",
    "    \n",
    "    # Read CSV with detected properties\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", str(props['has_header']).lower()) \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", props['delimiter']) \\\n",
    "        .option(\"quote\", props['quotechar']) \\\n",
    "        .option(\"encoding\", props['encoding']) \\\n",
    "        .load(file_path)\n",
    "    \n",
    "    print(f\"  Rows: {df.count()}\")\n",
    "    \n",
    "    # Sanitize column names\n",
    "    df = sanitize_dataframe_columns(df)\n",
    "    \n",
    "    # Validate schema\n",
    "    schema_valid, schema_warnings = validate_schema(df, domain, file_name)\n",
    "    if not schema_valid:\n",
    "        print(f\"  ⚠ Schema validation warnings:\")\n",
    "        for warning in schema_warnings:\n",
    "            print(f\"    - {warning}\")\n",
    "    \n",
    "    # Deduplicate\n",
    "    df, duplicates_removed = deduplicate_dataframe(df)\n",
    "    \n",
    "    # Validate data quality\n",
    "    quality_issues = validate_data_quality(df)\n",
    "    \n",
    "    # Perform detailed schema analysis\n",
    "    schema_analysis = analyze_dataframe_schema(df, file_name)\n",
    "    \n",
    "    # Add metadata columns\n",
    "    df = df.withColumn(\"_src_file\", lit(file_name))\n",
    "    df = df.withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "    df = df.withColumn(\"_execution_id\", lit(str(uuid.uuid4())))\n",
    "    # duplicates_removed tracked in metadata, not in bronze table\n",
    "    \n",
    "    # Trim strings\n",
    "    for field in df.schema.fields:\n",
    "        if isinstance(field.dataType, StringType):\n",
    "            df = df.withColumn(field.name, trim(col(field.name)))\n",
    "    \n",
    "    # Return df with metadata for audit logging\n",
    "    metadata = {\n",
    "        'file_props': props,\n",
    "        'schema_analysis': schema_analysis,\n",
    "        'duplicates_removed': duplicates_removed,\n",
    "        'quality_issues': quality_issues\n",
    "    }\n",
    "    \n",
    "    return df, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dcd29b4-34ff-460c-a5e3-b7249691678d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Column sanitization functions\n",
    "import re\n",
    "\n",
    "def sanitize_column_name(name: str) -> str:\n",
    "    \"\"\"Sanitize column name for Delta tables\"\"\"\n",
    "    sanitized = name.strip().lower()\n",
    "    sanitized = sanitized.replace(' ', '_')\n",
    "    sanitized = re.sub(r'[,;{}()\\[\\]]', '', sanitized)\n",
    "    sanitized = re.sub(r'[^a-zA-Z0-9_.$-]', '_', sanitized)\n",
    "    sanitized = re.sub(r'_+', '_', sanitized)\n",
    "    sanitized = sanitized.strip('_')\n",
    "    return sanitized\n",
    "\n",
    "def sanitize_dataframe_columns(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Sanitize all column names in DataFrame\"\"\"\n",
    "    original_cols = df.columns\n",
    "    new_columns = [sanitize_column_name(col) for col in original_cols]\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if len(new_columns) != len(set(new_columns)):\n",
    "        print(\"  ⚠ Warning: Duplicate column names after sanitization\")\n",
    "    \n",
    "    # Show what changed\n",
    "    changes = [(o, n) for o, n in zip(original_cols, new_columns) if o != n]\n",
    "    if changes:\n",
    "        print(f\"  Sanitized {len(changes)} column names\")\n",
    "    \n",
    "    return df.toDF(*new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96de6bae-f442-4615-830b-6a935aaf0f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data quality and deduplication functions\n",
    "\n",
    "def deduplicate_dataframe(df: DataFrame) -> tuple:\n",
    "    \"\"\"Remove duplicate rows and return df with count of duplicates removed\"\"\"\n",
    "    row_count_before = df.count()\n",
    "    df_deduped = df.dropDuplicates()\n",
    "    row_count_after = df_deduped.count()\n",
    "    duplicates_removed = row_count_before - row_count_after\n",
    "    \n",
    "    if duplicates_removed > 0:\n",
    "        print(f\"  Removed {duplicates_removed} duplicate rows\")\n",
    "    else:\n",
    "        print(f\"  No duplicates found\")\n",
    "    \n",
    "    return df_deduped, duplicates_removed\n",
    "\n",
    "def validate_data_quality(df: DataFrame) -> list:\n",
    "    \"\"\"Basic data quality checks\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    # Check if empty\n",
    "    row_count = df.count()\n",
    "    if row_count == 0:\n",
    "        issues.append(\"DataFrame is empty\")\n",
    "    \n",
    "    # Check for all-null columns\n",
    "    for col_name in df.columns:\n",
    "        if not col_name.startswith('_'):  # Skip metadata columns\n",
    "            non_null = df.select(col_name).filter(col(col_name).isNotNull()).count()\n",
    "            if non_null == 0:\n",
    "                issues.append(f\"Column '{col_name}' is all nulls\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  ⚠ Data quality issues: {'; '.join(issues)}\")\n",
    "    else:\n",
    "        print(f\"   Data quality checks passed\")\n",
    "    \n",
    "    return issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4433c13c-cf9f-496a-b9e0-6a63e64f8781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema Validation - Detects when MercuryGate changes export format\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Configure logger\n",
    "logger = logging.getLogger(\"SchemaValidator\")\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \"%Y-%m-%d %H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# Expected schemas by domain\n",
    "EXPECTED_SCHEMAS = {\n",
    "    \"Load Detail\": [\n",
    "        \"extract_timestamp\", \"extract_batch_id\", \"load_number\", \"load_created_date\",\n",
    "        \"customer_name\", \"origin_city\", \"origin_state\", \"destination_city\", \"destination_state\",\n",
    "        \"total_weight\", \"total_pieces\", \"equipment_type\", \"current_tender_status\",\n",
    "        \"current_tender_carrier\", \"current_tender_version\", \"current_tender_timestamp\",\n",
    "        \"tender_action_code\", \"tender_user\", \"load_status\", \"ready_date\", \"delivery_date\"\n",
    "    ],\n",
    "    \"Customer\": [\n",
    "        # Add customer schema columns here\n",
    "    ],\n",
    "    \"Carrier\": [\n",
    "        # Add carrier schema columns here\n",
    "    ]\n",
    "}\n",
    "\n",
    "def validate_schema(df: DataFrame, domain: str, file_name: str) -> tuple:\n",
    "    \"\"\"Validate DataFrame schema against expected schema for domain\"\"\"\n",
    "    \n",
    "    if domain not in EXPECTED_SCHEMAS or not EXPECTED_SCHEMAS[domain]:\n",
    "        logger.info(f\"No expected schema defined for domain: {domain}\")\n",
    "        return True, []\n",
    "    \n",
    "    expected_cols = set(EXPECTED_SCHEMAS[domain])\n",
    "    actual_cols = set(df.columns)\n",
    "    \n",
    "    # Remove metadata columns from comparison\n",
    "    actual_cols = {col for col in actual_cols if not col.startswith('_')}\n",
    "    \n",
    "    # Check for differences\n",
    "    missing_cols = expected_cols - actual_cols\n",
    "    extra_cols = actual_cols - expected_cols\n",
    "    \n",
    "    warnings = []\n",
    "    \n",
    "    if missing_cols:\n",
    "        msg = f\"Missing expected columns: {sorted(missing_cols)}\"\n",
    "        warnings.append(msg)\n",
    "        logger.warning(f\"{file_name}: {msg}\")\n",
    "    \n",
    "    if extra_cols:\n",
    "        msg = f\"Extra unexpected columns: {sorted(extra_cols)}\"\n",
    "        warnings.append(msg)\n",
    "        logger.warning(f\"{file_name}: {msg}\")\n",
    "    \n",
    "    if not warnings:\n",
    "        logger.info(f\"{file_name}: Schema validation passed\")\n",
    "        return True, []\n",
    "    \n",
    "    return False, warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24411522-1201-4f3a-a052-5aed7814b447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Advanced File Property Detection\n",
    "import chardet\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def detect_file_encoding(file_path: str, sample_size: int = 100000) -> str:\n",
    "    \"\"\"Detect file encoding using chardet\"\"\"\n",
    "    try:\n",
    "        # Read sample of file\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read(sample_size)\n",
    "        \n",
    "        result = chardet.detect(raw_data)\n",
    "        \n",
    "        if result and result['confidence'] > 0.7:\n",
    "            encoding = result['encoding']\n",
    "            print(f\"  Detected encoding: {encoding} (confidence: {result['confidence']:.2f})\")\n",
    "            return encoding\n",
    "        else:\n",
    "            print(f\"  Low confidence encoding detection, using UTF-8\")\n",
    "            return 'utf-8'\n",
    "    except Exception as e:\n",
    "        print(f\"  Error detecting encoding: {e}, using UTF-8\")\n",
    "        return 'utf-8'\n",
    "\n",
    "def detect_csv_properties(file_path: str, encoding: str = 'utf-8') -> dict:\n",
    "    \"\"\"Detect CSV delimiter, quote char, and other properties\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding=encoding) as f:\n",
    "            # Read first few lines\n",
    "            sample = ''.join([f.readline() for _ in range(5)])\n",
    "        \n",
    "        # Use CSV sniffer to detect dialect\n",
    "        sniffer = csv.Sniffer()\n",
    "        dialect = sniffer.sniff(sample)\n",
    "        has_header = sniffer.has_header(sample)\n",
    "        \n",
    "        properties = {\n",
    "            'encoding': encoding,\n",
    "            'delimiter': dialect.delimiter,\n",
    "            'quotechar': dialect.quotechar,\n",
    "            'has_header': has_header\n",
    "        }\n",
    "        \n",
    "        print(f\"  CSV Properties: delimiter={repr(properties['delimiter'])}, \"\n",
    "              f\"quote={repr(properties['quotechar'])}, header={properties['has_header']}\")\n",
    "        \n",
    "        return properties\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Could not detect CSV properties: {e}, using defaults\")\n",
    "        return {\n",
    "            'encoding': encoding,\n",
    "            'delimiter': ',',\n",
    "            'quotechar': '\"',\n",
    "            'has_header': True\n",
    "        }\n",
    "\n",
    "def detect_file_properties(file_path: str) -> dict:\n",
    "    \"\"\"Detect all file properties: encoding, delimiter, etc.\"\"\"\n",
    "    print(\"  Detecting file properties...\")\n",
    "    \n",
    "    # Detect encoding\n",
    "    encoding = detect_file_encoding(file_path)\n",
    "    \n",
    "    # Detect CSV properties\n",
    "    properties = detect_csv_properties(file_path, encoding)\n",
    "    \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1901084-65db-49c8-853d-1fbd2d7e5da3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detailed Schema Analysis\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def analyze_dataframe_schema(df: DataFrame, file_name: str) -> dict:\n",
    "    \"\"\"Perform detailed schema analysis with samples, nulls, and statistics\"\"\"\n",
    "    \n",
    "    print(f\"\\n  Analyzing schema for: {file_name}\")\n",
    "    print(\"  \" + \"-\" * 100)\n",
    "    \n",
    "    total_rows = df.count()\n",
    "    \n",
    "    # Calculate null counts for all columns in one pass\n",
    "    null_counts = df.select([\n",
    "        F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "    \n",
    "    # Calculate numeric sums\n",
    "    numeric_cols = []\n",
    "    for col_name, dtype in df.dtypes:\n",
    "        if any(t in dtype.lower() for t in ['int', 'long', 'double', 'float', 'decimal']):\n",
    "            numeric_cols.append(col_name)\n",
    "    \n",
    "    numeric_sums = {}\n",
    "    if numeric_cols:\n",
    "        sums_row = df.agg(*(F.sum(c).alias(c) for c in numeric_cols)).collect()[0]\n",
    "        numeric_sums = {c: float(sums_row[c]) if sums_row[c] is not None else 0 for c in numeric_cols}\n",
    "    \n",
    "    # Build schema details\n",
    "    schema_details = []\n",
    "    \n",
    "    print(f\"  {'Column':<30} {'Type':<12} {'Non-Null':<12} {'Null %':<8} {'Sum':<15} {'Samples'}\")\n",
    "    print(\"  \" + \"-\" * 100)\n",
    "    \n",
    "    for col_name, dtype in df.dtypes:\n",
    "        null_count = null_counts[col_name]\n",
    "        non_null = total_rows - null_count\n",
    "        null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "        \n",
    "        # Get sample values (limit to 3 for display)\n",
    "        samples = df.select(col_name).where(F.col(col_name).isNotNull()) \\\n",
    "            .distinct().limit(3).collect()\n",
    "        sample_str = ', '.join([str(row[0])[:20] for row in samples])\n",
    "        \n",
    "        # Format sum for numeric columns\n",
    "        sum_str = f\"{numeric_sums[col_name]:,.2f}\" if col_name in numeric_sums else \"\"\n",
    "        \n",
    "        print(f\"  {col_name:<30} {dtype:<12} {non_null:<12,} {null_pct:>6.1f}% {sum_str:<15} {sample_str}\")\n",
    "        \n",
    "        schema_details.append({\n",
    "            'column': col_name,\n",
    "            'type': dtype,\n",
    "            'non_null_count': non_null,\n",
    "            'null_count': null_count,\n",
    "            'null_percentage': null_pct,\n",
    "            'sum': numeric_sums.get(col_name),\n",
    "            'samples': [str(row[0]) for row in samples]\n",
    "        })\n",
    "    \n",
    "    print(\"  \" + \"-\" * 100)\n",
    "    print(f\"  Total Rows: {total_rows:,} | Total Columns: {len(df.columns)}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'total_rows': total_rows,\n",
    "        'total_columns': len(df.columns),\n",
    "        'schema_details': schema_details\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd70b039-5ded-4e80-9718-5001cf4072a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Performance metrics logging\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "import time\n",
    "\n",
    "def log_performance_metrics(file_name: str, file_size: int, row_count: int, \n",
    "                           duration_seconds: float, execution_id: str):\n",
    "    \"\"\"Log performance metrics\"\"\"\n",
    "    \n",
    "    metrics_table = \"dev_bronze.bronze_audit.performance_metrics\"\n",
    "    \n",
    "    # Create table if not exists\n",
    "    spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {metrics_table} (\n",
    "        execution_id STRING,\n",
    "        file_name STRING,\n",
    "        file_size_mb DOUBLE,\n",
    "        row_count LONG,\n",
    "        duration_seconds DOUBLE,\n",
    "        rows_per_second DOUBLE,\n",
    "        mb_per_second DOUBLE,\n",
    "        timestamp TIMESTAMP\n",
    "    ) USING DELTA\"\"\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    file_size_mb = file_size / (1024 * 1024) if file_size else 0\n",
    "    rows_per_sec = row_count / duration_seconds if duration_seconds > 0 else 0\n",
    "    mb_per_sec = file_size_mb / duration_seconds if duration_seconds > 0 else 0\n",
    "    \n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"file_size_mb\", DoubleType(), True),\n",
    "        StructField(\"row_count\", LongType(), True),\n",
    "        StructField(\"duration_seconds\", DoubleType(), True),\n",
    "        StructField(\"rows_per_second\", DoubleType(), True),\n",
    "        StructField(\"mb_per_second\", DoubleType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    metrics_df = spark.createDataFrame([\n",
    "        (execution_id, file_name, file_size_mb, row_count, duration_seconds, \n",
    "         rows_per_sec, mb_per_sec, datetime.now())\n",
    "    ], schema)\n",
    "    \n",
    "    metrics_df.write.mode(\"append\").saveAsTable(metrics_table)\n",
    "    \n",
    "    print(f\"   Performance: {row_count:,} rows in {duration_seconds:.2f}s ({rows_per_sec:.0f} rows/sec)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f667847-29c1-43f8-8ac3-549e353e86b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to bronze with smart schema evolution\n",
    "def write_to_bronze(df: DataFrame, domain: str):\n",
    "    \"\"\"Write DataFrame to bronze table based on domain with schema evolution\"\"\"\n",
    "    \n",
    "    # Get config for domain\n",
    "    config = get_bronze_config(domain)\n",
    "    table_name = f\"dev_bronze.{config['schema']}.{config['table']}\"\n",
    "    \n",
    "    print(f\"Writing to: {table_name}\")\n",
    "    \n",
    "    # Create schema if needed\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS dev_bronze.{config['schema']}\")\n",
    "    \n",
    "    # Check if table exists\n",
    "    table_exists = spark.catalog.tableExists(table_name)\n",
    "    \n",
    "    if table_exists:\n",
    "        # Get existing schema\n",
    "        existing_df = spark.table(table_name).limit(0)  # Just schema, no data\n",
    "        existing_cols = set(existing_df.columns)\n",
    "        new_cols = set(df.columns)\n",
    "        \n",
    "        # Check for schema changes\n",
    "        added_cols = new_cols - existing_cols\n",
    "        dropped_cols = existing_cols - new_cols\n",
    "        \n",
    "        if added_cols or dropped_cols:\n",
    "            print(f\" Schema changes detected:\")\n",
    "            \n",
    "            if added_cols:\n",
    "                print(f\" NEW columns (will be added): {sorted(added_cols)}\")\n",
    "                send_notification(\n",
    "                    message=f\"New columns detected in {domain}: {sorted(added_cols)}\",\n",
    "                    severity=\"INFO\",\n",
    "                    domain=domain\n",
    "                )\n",
    "            \n",
    "            if dropped_cols:\n",
    "                # Dropped columns are more concerning - send warning\n",
    "                print(f\" DROPPED columns (old data had these): {sorted(dropped_cols)}\")\n",
    "                send_notification(\n",
    "                    message=f\"WARNING: Columns dropped in {domain}: {sorted(dropped_cols)}\",\n",
    "                    severity=\"WARNING\",\n",
    "                    domain=domain\n",
    "                )\n",
    "        \n",
    "        # Write with mergeSchema enabled\n",
    "        df.write.mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n",
    "        print(f\"   Written {df.count()} rows (schema auto-merged)\")\n",
    "    else:\n",
    "        # First write - create table\n",
    "        df.write.mode(\"append\").saveAsTable(table_name)\n",
    "        print(f\" Written {df.count()} rows (table created)\")\n",
    "    \n",
    "    return table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e4705f-624c-4463-ae6c-2979dd07bba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced audit logging with complete details\n",
    "def log_to_audit(file_name: str, status: str, rows: int, execution_id: str, \n",
    "                 file_info: dict = None, schema_analysis: dict = None, \n",
    "                 file_props: dict = None, processing_notes: dict = None):\n",
    "    \"\"\"Log file processing to audit table with complete details\"\"\"\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, LongType, TimestampType\n",
    "    import json\n",
    "    \n",
    "    audit_table = \"dev_bronze.bronze_audit.file_processing_audit\"\n",
    "    \n",
    "    # Create schema\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS dev_bronze.bronze_audit\")\n",
    "    \n",
    "    # Create audit table\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {audit_table} (\n",
    "            execution_id STRING,\n",
    "            file_name STRING,\n",
    "            file_size LONG,\n",
    "            file_modified_timestamp TIMESTAMP,\n",
    "            start_time TIMESTAMP,\n",
    "            end_time TIMESTAMP,\n",
    "            user_name STRING,\n",
    "            status STRING,\n",
    "            rows_processed LONG,\n",
    "            schema_info STRING,\n",
    "            file_properties STRING,\n",
    "            processing_details STRING\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Get current user\n",
    "    try:\n",
    "        user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    except:\n",
    "        user = \"unknown\"\n",
    "    \n",
    "    # Extract file info\n",
    "    file_size = file_info.get('size') if file_info else None\n",
    "    file_modified = file_info.get('modified') if file_info else None\n",
    "    start_time = file_info.get('start_time') if file_info else datetime.now()\n",
    "    \n",
    "    # Convert complex objects to JSON\n",
    "    schema_json = json.dumps(schema_analysis) if schema_analysis else None\n",
    "    props_json = json.dumps(file_props) if file_props else None\n",
    "    details_json = json.dumps(processing_notes) if processing_notes else None\n",
    "    \n",
    "    # Define schema\n",
    "    schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"file_size\", LongType(), True),\n",
    "        StructField(\"file_modified_timestamp\", TimestampType(), True),\n",
    "        StructField(\"start_time\", TimestampType(), True),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"user_name\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"rows_processed\", LongType(), True),\n",
    "        StructField(\"schema_info\", StringType(), True),\n",
    "        StructField(\"file_properties\", StringType(), True),\n",
    "        StructField(\"processing_details\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Insert audit record\n",
    "    now = datetime.now()\n",
    "    \n",
    "    audit_df = spark.createDataFrame([\n",
    "        (execution_id, file_name, file_size, file_modified, start_time, now, \n",
    "         user, status, rows, schema_json, props_json, details_json)\n",
    "    ], schema)\n",
    "    \n",
    "    audit_df.write.mode(\"append\").saveAsTable(audit_table)\n",
    "    \n",
    "    print(f\"   Logged to audit table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6533c6c-1e30-4669-b394-1f61f9b0358b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notification system for operational alerts\n",
    "\n",
    "def send_notification(message: str, severity: str = \"INFO\", domain: str = None, execution_id: str = None):\n",
    "    \"\"\"Send notification for important events (errors, warnings, anomalies)\"\"\"\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "    \n",
    "    notifications_table = \"dev_bronze.bronze_audit.notifications\"\n",
    "    \n",
    "    # Create table if not exists\n",
    "    spark.sql(f\"\"\"CREATE TABLE IF NOT EXISTS {notifications_table} (\n",
    "        timestamp TIMESTAMP,\n",
    "        severity STRING,\n",
    "        message STRING,\n",
    "        domain STRING,\n",
    "        execution_id STRING\n",
    "    ) USING DELTA\"\"\")\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"severity\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"execution_id\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    notif_df = spark.createDataFrame([\n",
    "        (datetime.now(), severity, message, domain, execution_id)\n",
    "    ], schema)\n",
    "    \n",
    "    notif_df.write.mode(\"append\").saveAsTable(notifications_table)\n",
    "    \n",
    "    # Print with color coding\n",
    "    if severity == \"ERROR\":\n",
    "        print(f\"  \uD83D\uDD34 [{severity}] {message}\")\n",
    "    elif severity == \"WARNING\":\n",
    "        print(f\"  \uD83D\uDFE1 [{severity}] {message}\")\n",
    "    else:\n",
    "        print(f\"  ℹ️  [{severity}] {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77778b88-1efd-4fdc-82e4-db915c27badc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Archive file to domain-specific folder\n",
    "def archive_file(file_name: str, domain: str, status: str = \"SUCCESS\"):\n",
    "    \"\"\"Move file to processed or failed folder within domain\"\"\"\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Get domain folder\n",
    "    if domain in DOMAIN_FOLDER_MAP:\n",
    "        domain_folder = DOMAIN_FOLDER_MAP[domain]\n",
    "        source = f\"/mnt/{CONTAINER}/{domain_folder}/{PROCESSING_SUBFOLDERS['incoming']}/{file_name}\"\n",
    "        \n",
    "        # Archive to processed or failed\n",
    "        if status == \"SUCCESS\":\n",
    "            dest_folder = f\"{domain_folder}/{PROCESSING_SUBFOLDERS['processed']}\"\n",
    "        else:\n",
    "            dest_folder = f\"{domain_folder}/{PROCESSING_SUBFOLDERS['failed']}\"\n",
    "    else:\n",
    "        # Fallback to legacy\n",
    "        source = f\"/mnt/{CONTAINER}/{DOMAIN_FOLDER}/{INCOMING_FOLDER}/{file_name}\"\n",
    "        dest_folder = PROCESSED_FOLDER if status == \"SUCCESS\" else \"failed\"\n",
    "    \n",
    "    dest = f\"/mnt/{CONTAINER}/{dest_folder}/{date_str}/{file_name}\"\n",
    "    \n",
    "    # Create folder\n",
    "    dbutils.fs.mkdirs(f\"/mnt/{CONTAINER}/{dest_folder}/{date_str}\")\n",
    "    \n",
    "    # Move file\n",
    "    dbutils.fs.mv(source, dest)\n",
    "    \n",
    "    print(f\"   Archived to: {dest_folder}/{date_str}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "680f8c4f-4ee2-4a68-9e77-4a77d834cda1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FileProcessor Class - Wraps all processing logic\n",
    "\n",
    "class FileProcessor:\n",
    "    \"\"\"Processes files with all bronze layer features\"\"\"\n",
    "    \n",
    "    def __init__(self, execution_id: str = None):\n",
    "        self.execution_id = execution_id or str(uuid.uuid4())\n",
    "        self.start_time = time.time()\n",
    "        print(f\" FileProcessor initialized (Execution ID: {self.execution_id})\")\n",
    "    \n",
    "    def process_file(self, file_name: str, domain: str) -> tuple:\n",
    "        \"\"\"Process a single file and return (success, df, row_count, duration)\"\"\"\n",
    "        file_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Use the existing process_csv_file function\n",
    "            result = process_csv_file(file_name, domain)\n",
    "            \n",
    "            # Unpack the tuple (df, metadata)\n",
    "            if isinstance(result, tuple):\n",
    "                df, metadata = result\n",
    "            else:\n",
    "                # Fallback if not a tuple\n",
    "                df, metadata = result, {}\n",
    "            row_count = df.count()\n",
    "            duration = time.time() - file_start\n",
    "            \n",
    "            return (True, (df, metadata), row_count, duration)\n",
    "        except Exception as e:\n",
    "            print(f\" Error processing {file_name}: {e}\")\n",
    "            duration = time.time() - file_start\n",
    "            return (False, (None, {}), 0, duration)\n",
    "    \n",
    "    def process_batch(self, domain: str, file_pattern: str = None) -> dict:\n",
    "        \"\"\"Process all files in domain folder\"\"\"\n",
    "        \n",
    "        print(\"=\" * 100)\n",
    "        print(\"BRONZE LAYER BATCH INGESTION - FileProcessor\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Domain: {domain}\")\n",
    "        print(f\"File pattern: {file_pattern if file_pattern else 'All files'}\")\n",
    "        print(f\"Execution ID: {self.execution_id}\")\n",
    "        \n",
    "        # Get domain-specific incoming folder\n",
    "        if domain in DOMAIN_FOLDER_MAP:\n",
    "            domain_folder = DOMAIN_FOLDER_MAP[domain]\n",
    "            incoming_path = f\"/mnt/{CONTAINER}/{domain_folder}/{PROCESSING_SUBFOLDERS['incoming']}\"\n",
    "            print(f\"Using domain folder: {domain_folder}\")\n",
    "        else:\n",
    "            incoming_path = f\"/mnt/{CONTAINER}/{DOMAIN_FOLDER}/{INCOMING_FOLDER}\"\n",
    "            print(f\"Using legacy folder\")\n",
    "        \n",
    "        # List files\n",
    "        all_files = dbutils.fs.ls(incoming_path)\n",
    "        csv_files = [f for f in all_files if f.name.endswith('.csv') and not f.isDir()]\n",
    "        \n",
    "        if file_pattern:\n",
    "            csv_files = [f for f in csv_files if file_pattern in f.name]\n",
    "        \n",
    "        print(f\"\\nFound {len(csv_files)} CSV files\")\n",
    "        \n",
    "        # Get already processed files\n",
    "        try:\n",
    "            processed_files_df = spark.sql(\n",
    "                \"SELECT DISTINCT file_name FROM dev_bronze.bronze_audit.file_processing_audit \"\n",
    "                \"WHERE status = 'SUCCESS'\"\n",
    "            )\n",
    "            processed_files = set([row.file_name for row in processed_files_df.collect()])\n",
    "            print(f\"Already processed: {len(processed_files)} files\")\n",
    "        except:\n",
    "            processed_files = set()\n",
    "            print(\"No audit history found\")\n",
    "        \n",
    "        # Filter to unprocessed\n",
    "        files_to_process = [f for f in csv_files if f.name not in processed_files]\n",
    "        print(f\"Files to process: {len(files_to_process)}\")\n",
    "        \n",
    "        if len(files_to_process) == 0:\n",
    "            print(\"\\nAll files already processed!\")\n",
    "            return {'success_count': 0, 'fail_count': 0, 'table_name': None}\n",
    "        \n",
    "        # Process files\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"PROCESSING FILES\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        table_name = None\n",
    "        \n",
    "        for idx, file_info in enumerate(files_to_process, 1):\n",
    "            print(f\"\\n[{idx}/{len(files_to_process)}] Processing: {file_info.name}\")\n",
    "            print(\"-\" * 100)\n",
    "            \n",
    "            success, result_tuple, row_count, duration = self.process_file(file_info.name, domain)\n",
    "            \n",
    "            # Unpack result\n",
    "            if success and result_tuple:\n",
    "                df, metadata = result_tuple\n",
    "            else:\n",
    "                df, metadata = None, {}\n",
    "            \n",
    "            if success:\n",
    "                # Write to bronze\n",
    "                table_name = write_to_bronze(df, domain)\n",
    "                \n",
    "                # Archive\n",
    "                archive_file(file_info.name, domain, \"SUCCESS\")\n",
    "                \n",
    "                # Log performance\n",
    "                try:\n",
    "                    file_size = file_info.size\n",
    "                except:\n",
    "                    file_size = 0\n",
    "                log_performance_metrics(file_info.name, file_size, row_count, duration, self.execution_id)\n",
    "                \n",
    "                # Log audit\n",
    "                # Collect file metadata for audit\n",
    "                file_metadata = {\n",
    "                    'size': file_info.size,\n",
    "                    'modified': datetime.fromtimestamp(file_info.modificationTime / 1000),\n",
    "                    'start_time': datetime.now() - timedelta(seconds=duration)\n",
    "                }\n",
    "                \n",
    "                # Log with complete details\n",
    "                log_to_audit(\n",
    "                    file_name=file_info.name,\n",
    "                    status=\"SUCCESS\",\n",
    "                    rows=row_count,\n",
    "                    execution_id=self.execution_id,\n",
    "                    file_info=file_metadata,\n",
    "                    schema_analysis=metadata.get('schema_analysis'),\n",
    "                    file_props=metadata.get('file_props'),\n",
    "                    processing_notes={\n",
    "                        'duplicates_removed': metadata.get('duplicates_removed', 0),\n",
    "                        'quality_issues': metadata.get('quality_issues', [])\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Send notifications for issues\n",
    "                if metadata.get('quality_issues'):\n",
    "                    send_notification(\n",
    "                        message=f\"Data quality issues in {file_info.name}: {'; '.join(metadata['quality_issues'])}\",\n",
    "                        severity=\"WARNING\",\n",
    "                        domain=domain,\n",
    "                        execution_id=self.execution_id\n",
    "                    )\n",
    "                \n",
    "                print(f\"   SUCCESS\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                # Archive to failed\n",
    "                archive_file(file_info.name, domain, \"FAILED\")\n",
    "                log_to_audit(file_info.name, \"FAILED\", 0, self.execution_id)\n",
    "                \n",
    "                # Send error notification\n",
    "                send_notification(\n",
    "                    message=f\"Failed to process {file_info.name}\",\n",
    "                    severity=\"ERROR\",\n",
    "                    domain=domain,\n",
    "                    execution_id=self.execution_id\n",
    "                )\n",
    "                \n",
    "                fail_count += 1\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(\"BATCH PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"Successfully processed: {success_count}\")\n",
    "        print(f\"Failed: {fail_count}\")\n",
    "        \n",
    "        # Alert on high failure rate\n",
    "        if fail_count > 0 and fail_count >= success_count:\n",
    "            send_notification(\n",
    "                message=f\"High failure rate in {domain}: {fail_count} failed, {success_count} succeeded\",\n",
    "                severity=\"ERROR\",\n",
    "                domain=domain,\n",
    "                execution_id=self.execution_id\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'success_count': success_count,\n",
    "            'fail_count': fail_count,\n",
    "            'table_name': table_name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83937f96-3e8c-411c-9b28-72df8768b22c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " FileProcessor initialized (Execution ID: e9ae7fb7-83db-425d-83f0-d6288edb3379)\n====================================================================================================\nBRONZE LAYER BATCH INGESTION - FileProcessor\n====================================================================================================\nDomain: Load Detail\nFile pattern: All files\nExecution ID: e9ae7fb7-83db-425d-83f0-d6288edb3379\nUsing domain folder: raw_load_details\n\nFound 9 CSV files\nNo audit history found\nFiles to process: 9\n\n====================================================================================================\nPROCESSING FILES\n====================================================================================================\n\n[1/9] Processing: mg_gold_extract_20260105_1000.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1000.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1000.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 50\n2026-02-03 20:42:00 - INFO - mg_gold_extract_20260105_1000.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1000.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    50              0.0%                 2026-01-05 10:00:00\n  extract_batch_id               string       50              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       50              0.0%                 53901001LN, 53901000LN, 53901002LN\n  load_created_date              timestamp    50              0.0%                 2026-01-02 10:00:00, 2026-01-04 10:00:00, 2025-12-31 10:00:00\n  customer_name                  string       50              0.0%                 GLOBAL LOGISTICS CO, COASTAL FREIGHT LLC, GLASFLOSS INDUSTRIES\n  origin_city                    string       50              0.0%                 Phoenix, Houston, Fort Worth\n  origin_state                   string       50              0.0%                 AZ, MO, TX\n  destination_city               string       50              0.0%                 Phoenix, New York, Denver\n  destination_state              string       50              0.0%                 AZ, NY, CO\n  total_weight                   int          50              0.0% 1,351,450.00    39963, 11069, 39171\n  total_pieces                   int          50              0.0% 801.00          19, 11, 18\n  equipment_type                 string       50              0.0%                 Flatbed, Reefer, Van\n  current_tender_status          string       50              0.0%                 TENDERED, ACCEPTED, REJECTED\n  current_tender_carrier         string       50              0.0%                 ROADWAY EXPRESS, NATIONAL FREIGHT, RELIABLE TRANSPORT\n  current_tender_version         bigint       50              0.0% 8,838,051,225,000.00 176761001000, 176761000000, 176761002000\n  current_tender_timestamp       timestamp    50              0.0%                 2026-01-04 22:00:00, 2025-12-31 21:00:00, 2026-01-02 15:00:00\n  tender_action_code             string       50              0.0%                 A, X, R\n  tender_user                    string       50              0.0%                 cbell@glasfloss.com, dispatch@reliabletra, dispatch@roadwayexpr\n  load_status                    string       50              0.0%                 Cancelled, Active, Booked\n  ready_date                     timestamp    50              0.0%                 2026-01-04 10:00:00, 2026-01-03 10:00:00, 2026-01-06 10:00:00\n  delivery_date                  timestamp    50              0.0%                 2026-01-07 10:00:00, 2026-01-08 10:00:00, 2026-01-05 10:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 50 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n Written 50 rows (table created)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 50 rows in 20.26s (2 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[2/9] Processing: mg_gold_extract_20260105_1200.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1200.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1200.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 50\n2026-02-03 20:42:29 - INFO - mg_gold_extract_20260105_1200.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1200.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    50              0.0%                 2026-01-05 12:00:00\n  extract_batch_id               string       50              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       50              0.0%                 53902002LN, 53902001LN, 53902000LN\n  load_created_date              timestamp    50              0.0%                 2026-01-02 12:00:00, 2026-01-03 12:00:00, 2026-01-04 12:00:00\n  customer_name                  string       50              0.0%                 GLOBAL LOGISTICS CO, ACME MANUFACTURING, GLASFLOSS INDUSTRIES\n  origin_city                    string       50              0.0%                 Los Angeles, El Paso, Corpus Christi\n  origin_state                   string       50              0.0%                 CA, TN, TX\n  destination_city               string       50              0.0%                 Dallas, Los Angeles, Arlington\n  destination_state              string       50              0.0%                 CA, TN, TX\n  total_weight                   int          50              0.0% 1,346,086.00    34019, 32420, 41968\n  total_pieces                   int          50              0.0% 758.00          20, 9, 21\n  equipment_type                 string       50              0.0%                 Flatbed, Reefer, Van\n  current_tender_status          string       50              0.0%                 TENDERED, ACCEPTED, REJECTED\n  current_tender_carrier         string       50              0.0%                 PRIME LOGISTICS, ROADWAY EXPRESS, NATIONAL FREIGHT\n  current_tender_version         bigint       50              0.0% 8,838,101,225,000.00 176762002000, 176762001000, 176762000000\n  current_tender_timestamp       timestamp    50              0.0%                 2026-01-04 03:00:00, 2026-01-06 06:00:00, 2026-01-06 11:00:00\n  tender_action_code             string       50              0.0%                 T, A, X\n  tender_user                    string       50              0.0%                 cbell@glasfloss.com, dispatch@primelogist, dispatch@nationalfre\n  load_status                    string       50              0.0%                 Active, Delivered, Booked\n  ready_date                     timestamp    50              0.0%                 2026-01-03 12:00:00, 2026-01-06 12:00:00, 2026-01-07 12:00:00\n  delivery_date                  timestamp    50              0.0%                 2026-01-08 12:00:00, 2026-01-11 12:00:00, 2026-01-07 12:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 50 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n   Written 50 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 50 rows in 19.76s (3 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[3/9] Processing: mg_gold_extract_20260105_1220.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1220.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1220.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 3\n2026-02-03 20:42:54 - INFO - mg_gold_extract_20260105_1220.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1220.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    3               0.0%                 2026-01-05 12:20:00\n  extract_batch_id               string       3               0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       3               0.0%                 53920792LN, 53928729LN, 53913187LN\n  load_created_date              timestamp    3               0.0%                 2026-01-05 10:00:00, 2026-01-05 11:30:00, 2026-01-05 09:00:00\n  customer_name                  string       3               0.0%                 GLASFLOSS INDUSTRIES\n  origin_city                    string       3               0.0%                 Dallas, Austin, Fort Worth\n  origin_state                   string       3               0.0%                 TX\n  destination_city               string       3               0.0%                 San Antonio, El Paso, Houston\n  destination_state              string       3               0.0%                 TX\n  total_weight                   int          3               0.0% 74,700.00       32000, 18200, 24500\n  total_pieces                   int          3               0.0% 54.00           12, 24, 18\n  equipment_type                 string       3               0.0%                 Flatbed, Van\n  current_tender_status          string       3               0.0%                 TENDERED, ACCEPTED\n  current_tender_carrier         string       3               0.0%                 BECKER LOGISTICS, RELIABLE TRANSPORT, SPOT FREIGHT INC\n  current_tender_version         bigint       3               0.0% 530,289,493,773.00 176763370316, 176762000001, 176764123456\n  current_tender_timestamp       timestamp    3               0.0%                 2026-01-05 12:10:00, 2026-01-05 12:15:30, 2026-01-05 09:30:00\n  tender_action_code             string       3               0.0%                 T, A\n  tender_user                    string       3               0.0%                 cbell@glasfloss.com, dispatch@reliabletra\n  load_status                    string       3               0.0%                 Active, Booked\n  ready_date                     timestamp    3               0.0%                 2026-01-06 14:00:00, 2026-01-06 08:00:00, 2026-01-06 06:00:00\n  delivery_date                  timestamp    3               0.0%                 2026-01-08 17:00:00, 2026-01-07 18:00:00, 2026-01-07 17:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 3 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n   Written 3 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 3 rows in 17.95s (0 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[4/9] Processing: mg_gold_extract_20260105_1240.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1240.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1240.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 3\n2026-02-03 20:43:17 - INFO - mg_gold_extract_20260105_1240.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1240.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    3               0.0%                 2026-01-05 12:40:00\n  extract_batch_id               string       3               0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       3               0.0%                 53920792LN, 53928729LN, 53913187LN\n  load_created_date              timestamp    3               0.0%                 2026-01-05 10:00:00, 2026-01-05 11:30:00, 2026-01-05 09:00:00\n  customer_name                  string       3               0.0%                 GLASFLOSS INDUSTRIES\n  origin_city                    string       3               0.0%                 Dallas, Austin, Fort Worth\n  origin_state                   string       3               0.0%                 TX\n  destination_city               string       3               0.0%                 San Antonio, El Paso, Houston\n  destination_state              string       3               0.0%                 TX\n  total_weight                   int          3               0.0% 74,700.00       32000, 18200, 24500\n  total_pieces                   int          3               0.0% 54.00           12, 24, 18\n  equipment_type                 string       3               0.0%                 Flatbed, Van\n  current_tender_status          string       3               0.0%                 ACCEPTED\n  current_tender_carrier         string       3               0.0%                 RELIABLE TRANSPORT, BECKER LOGISTICS 221, SPOT FREIGHT INC\n  current_tender_version         bigint       3               0.0% 530,290,177,332.00 176762000001, 176764555777, 176763621554\n  current_tender_timestamp       timestamp    3               0.0%                 2026-01-05 12:28:45, 2026-01-05 12:35:10, 2026-01-05 09:30:00\n  tender_action_code             string       3               0.0%                 A\n  tender_user                    string       3               0.0%                 dispatch@spotfreight, phil.michael@beckerl, dispatch@reliabletra\n  load_status                    string       3               0.0%                 Booked\n  ready_date                     timestamp    3               0.0%                 2026-01-06 14:00:00, 2026-01-06 08:00:00, 2026-01-06 06:00:00\n  delivery_date                  timestamp    3               0.0%                 2026-01-08 17:00:00, 2026-01-07 18:00:00, 2026-01-07 17:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 3 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n   Written 3 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 3 rows in 17.98s (0 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[5/9] Processing: mg_gold_extract_20260105_1400.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1400.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1400.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 50\n2026-02-03 20:43:39 - INFO - mg_gold_extract_20260105_1400.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1400.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    50              0.0%                 2026-01-05 14:00:00\n  extract_batch_id               string       50              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       50              0.0%                 53903000LN, 53903001LN, 53903002LN\n  load_created_date              timestamp    50              0.0%                 2026-01-03 14:00:00, 2025-12-31 14:00:00, 2026-01-04 14:00:00\n  customer_name                  string       50              0.0%                 GLOBAL LOGISTICS CO, ACME MANUFACTURING, COASTAL FREIGHT LLC\n  origin_city                    string       50              0.0%                 Chicago, Corpus Christi, Houston\n  origin_state                   string       50              0.0%                 IL, NY, TX\n  destination_city               string       50              0.0%                 Phoenix, Dallas, New York\n  destination_state              string       50              0.0%                 AZ, NY, TX\n  total_weight                   int          50              0.0% 1,358,721.00    16704, 43462, 36730\n  total_pieces                   int          50              0.0% 837.00          28, 8, 7\n  equipment_type                 string       50              0.0%                 Flatbed, Tanker, LTL\n  current_tender_status          string       50              0.0%                 TENDERED, ACCEPTED, REJECTED\n  current_tender_carrier         string       50              0.0%                 BECKER LOGISTICS, ROADWAY EXPRESS, RELIABLE TRANSPORT\n  current_tender_version         bigint       50              0.0% 8,838,151,225,000.00 176763000000, 176763002000, 176763001000\n  current_tender_timestamp       timestamp    50              0.0%                 2026-01-05 13:00:00, 2026-01-01 07:00:00, 2026-01-04 02:00:00\n  tender_action_code             string       50              0.0%                 T, A, R\n  tender_user                    string       50              0.0%                 jsmith@acme.com, dispatch@reliabletra, dispatch@roadwayexpr\n  load_status                    string       50              0.0%                 Delivered, Pending, Booked\n  ready_date                     timestamp    50              0.0%                 2026-01-02 14:00:00, 2026-01-05 14:00:00, 2026-01-06 14:00:00\n  delivery_date                  timestamp    50              0.0%                 2026-01-09 14:00:00, 2026-01-10 14:00:00, 2026-01-04 14:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 50 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n   Written 50 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 50 rows in 18.19s (3 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[6/9] Processing: mg_gold_extract_20260105_1600.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1600.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1600.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 50\n2026-02-03 20:44:02 - INFO - mg_gold_extract_20260105_1600.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1600.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    50              0.0%                 2026-01-05 16:00:00\n  extract_batch_id               string       50              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       50              0.0%                 53904001LN, 53904002LN, 53904000LN\n  load_created_date              timestamp    50              0.0%                 2025-12-31 16:00:00, 2026-01-02 16:00:00, 2026-01-03 16:00:00\n  customer_name                  string       50              0.0%                 ACME MANUFACTURING, PRECISION PARTS INC, MIDWEST DISTRIBUTION\n  origin_city                    string       50              0.0%                 Dallas, San Antonio, Arlington\n  origin_state                   string       50              0.0%                 AZ, NY, TX\n  destination_city               string       50              0.0%                 Austin, Kansas City, Fort Worth\n  destination_state              string       50              0.0%                 CA, MO, TX\n  total_weight                   int          50              0.0% 1,331,684.00    5335, 11070, 36065\n  total_pieces                   int          50              0.0% 810.00          6, 15, 18\n  equipment_type                 string       50              0.0%                 Flatbed, Reefer, Tanker\n  current_tender_status          string       50              0.0%                 TENDERED, ACCEPTED, REJECTED\n  current_tender_carrier         string       50              0.0%                 BECKER LOGISTICS, SPOT FREIGHT INC, SWIFT CARRIERS\n  current_tender_version         bigint       50              0.0% 8,838,201,225,000.00 176764000000, 176764002000, 176764001000\n  current_tender_timestamp       timestamp    50              0.0%                 2026-01-01 22:00:00, 2026-01-05 15:00:00, 2026-01-04 08:00:00\n  tender_action_code             string       50              0.0%                 T, A, X\n  tender_user                    string       50              0.0%                 jsmith@acme.com, dispatch@beckerlogis, dispatch@spotfreight\n  load_status                    string       50              0.0%                 Cancelled, Delivered, Pending\n  ready_date                     timestamp    50              0.0%                 2026-01-01 16:00:00, 2026-01-04 16:00:00, 2026-01-05 16:00:00\n  delivery_date                  timestamp    50              0.0%                 2026-01-07 16:00:00, 2026-01-04 16:00:00, 2026-01-06 16:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 50 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n   Written 50 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 50 rows in 18.54s (3 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[7/9] Processing: mg_gold_extract_20260105_1800.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_20260105_1800.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_20260105_1800.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 50\n2026-02-03 20:44:26 - INFO - mg_gold_extract_20260105_1800.csv: Schema validation passed\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_20260105_1800.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    50              0.0%                 2026-01-05 18:00:00\n  extract_batch_id               string       50              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       50              0.0%                 53905001LN, 53905000LN, 53905002LN\n  load_created_date              timestamp    50              0.0%                 2026-01-03 18:00:00, 2025-12-31 18:00:00, 2026-01-04 18:00:00\n  customer_name                  string       50              0.0%                 GLOBAL LOGISTICS CO, COASTAL FREIGHT LLC, GLASFLOSS INDUSTRIES\n  origin_city                    string       50              0.0%                 Dallas, El Paso, Corpus Christi\n  origin_state                   string       50              0.0%                 IL, TX, GA\n  destination_city               string       50              0.0%                 Phoenix, Kansas City, Fort Worth\n  destination_state              string       50              0.0%                 AZ, MO, TX\n  total_weight                   int          50              0.0% 1,292,598.00    28243, 10806, 24304\n  total_pieces                   int          50              0.0% 963.00          28, 26, 12\n  equipment_type                 string       50              0.0%                 Reefer, Tanker, Van\n  current_tender_status          string       50              0.0%                 EXPIRED, TENDERED, ACCEPTED\n  current_tender_carrier         string       50              0.0%                 BECKER LOGISTICS, ROADWAY EXPRESS, SPOT FREIGHT INC\n  current_tender_version         bigint       50              0.0% 8,838,251,225,000.00 176765002000, 176765001000, 176765000000\n  current_tender_timestamp       timestamp    50              0.0%                 2026-01-01 01:00:00, 2026-01-06 02:00:00, 2026-01-05 08:00:00\n  tender_action_code             string       50              0.0%                 T, A, X\n  tender_user                    string       50              0.0%                 cbell@glasfloss.com, dispatch@beckerlogis, dispatch@spotfreight\n  load_status                    string       50              0.0%                 Active, Delivered, Booked\n  ready_date                     timestamp    50              0.0%                 2026-01-07 18:00:00, 2026-01-01 18:00:00, 2026-01-06 18:00:00\n  delivery_date                  timestamp    50              0.0%                 2026-01-10 18:00:00, 2026-01-08 18:00:00, 2026-01-05 18:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 50 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n   Written 50 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 50 rows in 17.68s (3 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[8/9] Processing: mg_gold_extract_schema_change.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_schema_change.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_schema_change.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 15\n2026-02-03 20:44:49 - WARNING - mg_gold_extract_schema_change.csv: Extra unexpected columns: ['new_column_added_by_mercurygate']\n  ⚠ Schema validation warnings:\n    - Extra unexpected columns: ['new_column_added_by_mercurygate']\n  No duplicates found\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_schema_change.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    15              0.0%                 2026-01-05 16:00:00\n  extract_batch_id               string       15              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       15              0.0%                 54000001LN, 54000002LN, 54000000LN\n  load_created_date              timestamp    15              0.0%                 2026-01-05 12:00:00\n  customer_name                  string       15              0.0%                 SCHEMA TEST CUSTOMER\n  origin_city                    string       15              0.0%                 Austin\n  origin_state                   string       15              0.0%                 TX\n  destination_city               string       15              0.0%                 Dallas\n  destination_state              string       15              0.0%                 TX\n  total_weight                   int          15              0.0% 450,000.00      30000\n  total_pieces                   int          15              0.0% 300.00          20\n  equipment_type                 string       15              0.0%                 Flatbed\n  current_tender_status          string       15              0.0%                 TENDERED\n  current_tender_carrier         string       15              0.0%                 NEW CARRIER\n  current_tender_version         bigint       15              0.0% 2,651,700,000,000.00 176780000000\n  current_tender_timestamp       timestamp    15              0.0%                 2026-01-05 13:00:00\n  tender_action_code             string       15              0.0%                 T\n  tender_user                    string       15              0.0%                 dispatch@newcarrier.\n  load_status                    string       15              0.0%                 Pending\n  ready_date                     timestamp    15              0.0%                 2026-01-06 10:00:00\n  delivery_date                  timestamp    15              0.0%                 2026-01-08 16:00:00\n  new_column_added_by_mercurygate string       15              0.0%                 NEW_FIELD_VALUE\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 15 | Total Columns: 22\n\nWriting to: dev_bronze.load_detail.load_transactions\n Schema changes detected:\n NEW columns (will be added): ['new_column_added_by_mercurygate']\n  ℹ️  [INFO] New columns detected in Load Detail: ['new_column_added_by_mercurygate']\n   Written 15 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 15 rows in 18.44s (1 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n[9/9] Processing: mg_gold_extract_with_duplicates.csv\n----------------------------------------------------------------------------------------------------\nReading: mg_gold_extract_with_duplicates.csv\n  Path: /mnt/mg-gold-raw-files/raw_load_details/incoming/mg_gold_extract_with_duplicates.csv\n  Detecting file properties...\n  Detected encoding: ascii (confidence: 1.00)\n  CSV Properties: delimiter=',', quote='\"', header=True\n  Rows: 25\n2026-02-03 20:45:15 - INFO - mg_gold_extract_with_duplicates.csv: Schema validation passed\n  Removed 5 duplicate rows\n   Data quality checks passed\n\n  Analyzing schema for: mg_gold_extract_with_duplicates.csv\n  ----------------------------------------------------------------------------------------------------\n  Column                         Type         Non-Null     Null %   Sum             Samples\n  ----------------------------------------------------------------------------------------------------\n  extract_timestamp              timestamp    20              0.0%                 2026-01-05 14:00:00\n  extract_batch_id               string       20              0.0%                 MG_EXTRACT_20260105_\n  load_number                    string       20              0.0%                 53990001LN, 53990002LN, 53990000LN\n  load_created_date              timestamp    20              0.0%                 2026-01-05 10:00:00\n  customer_name                  string       20              0.0%                 TEST CUSTOMER\n  origin_city                    string       20              0.0%                 Dallas\n  origin_state                   string       20              0.0%                 TX\n  destination_city               string       20              0.0%                 Houston\n  destination_state              string       20              0.0%                 TX\n  total_weight                   int          20              0.0% 500,000.00      25000\n  total_pieces                   int          20              0.0% 300.00          15\n  equipment_type                 string       20              0.0%                 Van\n  current_tender_status          string       20              0.0%                 ACCEPTED\n  current_tender_carrier         string       20              0.0%                 TEST CARRIER\n  current_tender_version         bigint       20              0.0% 3,535,400,000,000.00 176770000000\n  current_tender_timestamp       timestamp    20              0.0%                 2026-01-05 11:00:00\n  tender_action_code             string       20              0.0%                 A\n  tender_user                    string       20              0.0%                 test@test.com\n  load_status                    string       20              0.0%                 Booked\n  ready_date                     timestamp    20              0.0%                 2026-01-06 08:00:00\n  delivery_date                  timestamp    20              0.0%                 2026-01-07 17:00:00\n  ----------------------------------------------------------------------------------------------------\n  Total Rows: 20 | Total Columns: 21\n\nWriting to: dev_bronze.load_detail.load_transactions\n Schema changes detected:\n DROPPED columns (old data had these): ['new_column_added_by_mercurygate']\n  \uD83D\uDFE1 [WARNING] WARNING: Columns dropped in Load Detail: ['new_column_added_by_mercurygate']\n   Written 20 rows (schema auto-merged)\n   Archived to: raw_load_details/processed/20260203/\n   Performance: 20 rows in 17.61s (1 rows/sec)\n   Logged to audit table\n   SUCCESS\n\n====================================================================================================\nBATCH PROCESSING COMPLETE\n====================================================================================================\nSuccessfully processed: 9\nFailed: 0\n\nBronze table contents:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>extract_timestamp</th><th>extract_batch_id</th><th>load_number</th><th>load_created_date</th><th>customer_name</th><th>origin_city</th><th>origin_state</th><th>destination_city</th><th>destination_state</th><th>total_weight</th><th>total_pieces</th><th>equipment_type</th><th>current_tender_status</th><th>current_tender_carrier</th><th>current_tender_version</th><th>current_tender_timestamp</th><th>tender_action_code</th><th>tender_user</th><th>load_status</th><th>ready_date</th><th>delivery_date</th><th>_src_file</th><th>_ingestion_timestamp</th><th>_execution_id</th><th>new_column_added_by_mercurygate</th></tr></thead><tbody><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990004LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990015LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990017LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990002LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990009LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990013LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990007LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990014LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990011LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990008LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990005LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990018LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990019LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990001LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990016LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990003LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990006LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990010LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990000LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr><tr><td>2026-01-05T14:00:00Z</td><td>MG_EXTRACT_20260105_1400</td><td>53990012LN</td><td>2026-01-05T10:00:00Z</td><td>TEST CUSTOMER</td><td>Dallas</td><td>TX</td><td>Houston</td><td>TX</td><td>25000</td><td>15</td><td>Van</td><td>ACCEPTED</td><td>TEST CARRIER</td><td>176770000000</td><td>2026-01-05T11:00:00Z</td><td>A</td><td>test@test.com</td><td>Booked</td><td>2026-01-06T08:00:00Z</td><td>2026-01-07T17:00:00Z</td><td>mg_gold_extract_with_duplicates.csv</td><td>2026-02-03T20:45:33.534577Z</td><td>4ef76ac4-8880-43c9-b675-12dae212a294</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990004LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990015LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990017LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990002LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990009LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990013LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990007LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990014LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990011LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990008LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990005LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990018LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990019LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990001LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990016LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990003LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990006LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990010LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990000LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ],
        [
         "2026-01-05T14:00:00Z",
         "MG_EXTRACT_20260105_1400",
         "53990012LN",
         "2026-01-05T10:00:00Z",
         "TEST CUSTOMER",
         "Dallas",
         "TX",
         "Houston",
         "TX",
         25000,
         15,
         "Van",
         "ACCEPTED",
         "TEST CARRIER",
         176770000000,
         "2026-01-05T11:00:00Z",
         "A",
         "test@test.com",
         "Booked",
         "2026-01-06T08:00:00Z",
         "2026-01-07T17:00:00Z",
         "mg_gold_extract_with_duplicates.csv",
         "2026-02-03T20:45:33.534577Z",
         "4ef76ac4-8880-43c9-b675-12dae212a294",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "extract_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "extract_batch_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "load_number",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "load_created_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "origin_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "origin_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_city",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_weight",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "total_pieces",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "equipment_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "current_tender_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "current_tender_carrier",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "current_tender_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "current_tender_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "tender_action_code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tender_user",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "load_status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ready_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "delivery_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "_src_file",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_ingestion_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "_execution_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "new_column_added_by_mercurygate",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nAudit table:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>file_name</th><th>status</th><th>rows_processed</th><th>start_time</th></tr></thead><tbody><tr><td>mg_gold_extract_with_duplicates.csv</td><td>SUCCESS</td><td>20</td><td>2026-02-03T20:45:20.368594Z</td></tr><tr><td>mg_gold_extract_schema_change.csv</td><td>SUCCESS</td><td>15</td><td>2026-02-03T20:44:54.387008Z</td></tr><tr><td>mg_gold_extract_20260105_1800.csv</td><td>SUCCESS</td><td>50</td><td>2026-02-03T20:44:29.484884Z</td></tr><tr><td>mg_gold_extract_20260105_1600.csv</td><td>SUCCESS</td><td>50</td><td>2026-02-03T20:44:05.447454Z</td></tr><tr><td>mg_gold_extract_20260105_1400.csv</td><td>SUCCESS</td><td>50</td><td>2026-02-03T20:43:42.344011Z</td></tr><tr><td>mg_gold_extract_20260105_1240.csv</td><td>SUCCESS</td><td>3</td><td>2026-02-03T20:43:19.506423Z</td></tr><tr><td>mg_gold_extract_20260105_1220.csv</td><td>SUCCESS</td><td>3</td><td>2026-02-03T20:42:56.729188Z</td></tr><tr><td>mg_gold_extract_20260105_1200.csv</td><td>SUCCESS</td><td>50</td><td>2026-02-03T20:42:32.303986Z</td></tr><tr><td>mg_gold_extract_20260105_1000.csv</td><td>SUCCESS</td><td>50</td><td>2026-02-03T20:42:05.982885Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "mg_gold_extract_with_duplicates.csv",
         "SUCCESS",
         20,
         "2026-02-03T20:45:20.368594Z"
        ],
        [
         "mg_gold_extract_schema_change.csv",
         "SUCCESS",
         15,
         "2026-02-03T20:44:54.387008Z"
        ],
        [
         "mg_gold_extract_20260105_1800.csv",
         "SUCCESS",
         50,
         "2026-02-03T20:44:29.484884Z"
        ],
        [
         "mg_gold_extract_20260105_1600.csv",
         "SUCCESS",
         50,
         "2026-02-03T20:44:05.447454Z"
        ],
        [
         "mg_gold_extract_20260105_1400.csv",
         "SUCCESS",
         50,
         "2026-02-03T20:43:42.344011Z"
        ],
        [
         "mg_gold_extract_20260105_1240.csv",
         "SUCCESS",
         3,
         "2026-02-03T20:43:19.506423Z"
        ],
        [
         "mg_gold_extract_20260105_1220.csv",
         "SUCCESS",
         3,
         "2026-02-03T20:42:56.729188Z"
        ],
        [
         "mg_gold_extract_20260105_1200.csv",
         "SUCCESS",
         50,
         "2026-02-03T20:42:32.303986Z"
        ],
        [
         "mg_gold_extract_20260105_1000.csv",
         "SUCCESS",
         50,
         "2026-02-03T20:42:05.982885Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "file_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "rows_processed",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "start_time",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main execution using FileProcessor class\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "# Get widget values\n",
    "widget_values = WidgetManager.get_widget_values()\n",
    "domain = widget_values.get(\"processing_option\", \"Load Detail\")\n",
    "file_pattern = widget_values.get(\"file_pattern\", \"\")\n",
    "\n",
    "# Create FileProcessor and run batch processing\n",
    "processor = FileProcessor()\n",
    "results = processor.process_batch(domain, file_pattern)\n",
    "\n",
    "# Display results if files were processed\n",
    "if results['table_name']:\n",
    "    print(\"\\nBronze table contents:\")\n",
    "    display(spark.table(results['table_name']).orderBy(col(\"_ingestion_timestamp\").desc()).limit(20))\n",
    "    \n",
    "    print(\"\\nAudit table:\")\n",
    "    display(spark.sql(\"\"\"\n",
    "        SELECT file_name, status, rows_processed, start_time \n",
    "        FROM dev_bronze.bronze_audit.file_processing_audit \n",
    "        ORDER BY start_time DESC LIMIT 10\n",
    "    \"\"\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "nb_bronze_ingestion",
   "widgets": {
    "carrier_id": {
     "currentValue": "",
     "nuid": "2f163bb3-e179-48b0-900e-8cf0f41b7925",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Carrier ID (optional)",
      "name": "carrier_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Carrier ID (optional)",
      "name": "carrier_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "customer_id": {
     "currentValue": "",
     "nuid": "f51da0f9-2ad4-4f06-a8a4-c82896b07f5b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Customer ID (optional)",
      "name": "customer_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Customer ID (optional)",
      "name": "customer_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "file_pattern": {
     "currentValue": "",
     "nuid": "0507858c-f8bf-43fd-83ea-15e45d2d72c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "File Pattern (optional)",
      "name": "file_pattern",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "File Pattern (optional)",
      "name": "file_pattern",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "processing_option": {
     "currentValue": "Load Detail",
     "nuid": "80896e53-e4d9-4b8c-84ab-c35a67e532d7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Load Detail",
      "label": "Data Domain",
      "name": "processing_option",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Customer",
        "Carrier",
        "Load Detail"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Load Detail",
      "label": "Data Domain",
      "name": "processing_option",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "Customer",
        "Carrier",
        "Load Detail"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}