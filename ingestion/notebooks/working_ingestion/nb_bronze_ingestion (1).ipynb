{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb86e4c-c3b2-4c60-9ad9-f73c3636460a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Bronze Layer Ingestion - Spark Parallel Processing\n",
    "# ============================================================================\n",
    "# Purpose: High-performance batch ingestion using Spark distributed processing\n",
    "# Author: Data Engineering Team\n",
    "# Version: 3.0.0\n",
    "# \n",
    "# Features:\n",
    "# - Parallel file processing using Spark distributed engine\n",
    "# - Dynamic schema loading from central registry\n",
    "# - Complete audit trail (batch and per-file)\n",
    "# - Performance and cost tracking\n",
    "# - Automated archival and cleanup\n",
    "# \n",
    "# Performance: 10-20x faster than serial processing for large batches\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "886a4984-8234-4644-8865-150732b1ccef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Spark Configuration\n",
    "\n",
    "def configure_spark():\n",
    "    \"\"\"Configure Spark settings - compatible with both classic and serverless\"\"\"\n",
    "    \n",
    "    configs = {\n",
    "        \"spark.sql.adaptive.enabled\": \"true\",\n",
    "        \"spark.sql.adaptive.coalescePartitions.enabled\": \"true\",\n",
    "        \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "        \"spark.sql.csv.enableVectorizedReader\": \"true\",\n",
    "        \"spark.databricks.delta.autoCompact.enabled\": \"true\",\n",
    "        \"spark.sql.execution.arrow.pyspark.enabled\": \"true\",\n",
    "        \"spark.sql.files.maxPartitionBytes\": \"128MB\",\n",
    "    }\n",
    "    \n",
    "    for key, value in configs.items():\n",
    "        try:\n",
    "            spark.conf.set(key, value)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "configure_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbb81d1-6ca8-4504-b808-7ae6d9496d26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, current_timestamp, trim, to_date, \n",
    "    input_file_name, regexp_extract, row_number, when, count, sum as _sum\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c74b2bb2-a1cf-499f-ba16-6effac0b6a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "CONTAINER = \"mg-gold-raw-files\"\n",
    "\n",
    "DOMAIN_FOLDER_MAP = {\n",
    "    \"Load Detail\": \"raw_load_details\"\n",
    "}\n",
    "\n",
    "BRONZE_TABLE_CONFIG = {\n",
    "    \"Load Detail\": {\n",
    "        \"schema\": \"load_detail\",\n",
    "        \"table\": \"load_transactions\"\n",
    "    }\n",
    "}\n",
    "\n",
    "COST_CONFIG = {\n",
    "    \"vm_costs\": {\n",
    "        \"standard_ds3_v2\": 0.192,\n",
    "        \"standard_ds4_v2\": 0.384,\n",
    "        \"standard_ds5_v2\": 0.768,\n",
    "        \"standard_e4s_v3\": 0.252,\n",
    "        \"standard_e8s_v3\": 0.504,\n",
    "        \"standard_f4s\": 0.169,\n",
    "        \"standard_f8s\": 0.338,\n",
    "        \"unknown\": 0.20\n",
    "    },\n",
    "    \"dbu_costs\": {\n",
    "        \"serverless\": 0.07,\n",
    "        \"jobs_compute\": 0.10,\n",
    "        \"all_purpose\": 0.40,\n",
    "        \"sql_warehouse\": 0.22,\n",
    "        \"dlt_pipeline\": 0.20,\n",
    "        \"classic\": 0.15,\n",
    "        \"unknown\": 0.15\n",
    "    },\n",
    "    \"storage_costs\": {\n",
    "        \"hot_tier\": 0.0208,\n",
    "        \"cool_tier\": 0.0115,\n",
    "        \"archive_tier\": 0.002\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_bronze_config(domain: str) -> dict:\n",
    "    if domain not in BRONZE_TABLE_CONFIG:\n",
    "        raise ValueError(f\"Unknown domain: {domain}\")\n",
    "    return BRONZE_TABLE_CONFIG[domain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5261f8-1e7b-4334-8079-4394562b4b98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schema Registry Functions\n",
    "\n",
    "def json_to_spark_schema(schema_json: str) -> StructType:\n",
    "    \"\"\"Convert JSON schema string to PySpark StructType\"\"\"\n",
    "    \n",
    "    schema_dict = json.loads(schema_json)\n",
    "    \n",
    "    def dict_to_field(field_dict: dict) -> StructField:\n",
    "        type_str = field_dict['type']\n",
    "        \n",
    "        type_mapping = {\n",
    "            'StringType()': StringType(),\n",
    "            'IntegerType()': IntegerType(),\n",
    "            'LongType()': LongType(),\n",
    "            'DoubleType()': DoubleType(),\n",
    "            'FloatType()': FloatType(),\n",
    "            'BooleanType()': BooleanType(),\n",
    "            'TimestampType()': TimestampType(),\n",
    "            'DateType()': DateType(),\n",
    "        }\n",
    "        \n",
    "        field_type = type_mapping.get(type_str, StringType())\n",
    "        \n",
    "        return StructField(\n",
    "            name=field_dict['name'],\n",
    "            dataType=field_type,\n",
    "            nullable=field_dict['nullable']\n",
    "        )\n",
    "    \n",
    "    fields = [dict_to_field(f) for f in schema_dict['fields']]\n",
    "    return StructType(fields)\n",
    "\n",
    "def get_domain_schema(domain: str) -> dict:\n",
    "    \"\"\"Retrieve active schema configuration from registry\"\"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "        SELECT * FROM dev_bronze.metadata.schema_registry\n",
    "        WHERE domain = '{domain}' AND is_active = true\n",
    "        ORDER BY schema_version DESC\n",
    "        LIMIT 1\n",
    "    \"\"\"\n",
    "    \n",
    "    result = spark.sql(query).collect()\n",
    "    \n",
    "    if not result:\n",
    "        raise ValueError(f\"No active schema found for domain: {domain}\")\n",
    "    \n",
    "    row = result[0]\n",
    "    schema = json_to_spark_schema(row.schema_json)\n",
    "    \n",
    "    return {\n",
    "        'domain': row.domain,\n",
    "        'version': row.schema_version,\n",
    "        'schema': schema,\n",
    "        'file_pattern': row.file_pattern,\n",
    "        'partition_by': row.partition_by,\n",
    "        'zorder_by': row.zorder_by.split(',') if row.zorder_by else [],\n",
    "        'description': row.description\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa3b249-c462-409d-8696-f20a31daa404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Widgets\n",
    "\n",
    "dbutils.widgets.dropdown(\"domain\", \"Load Detail\", [\"Load Detail\"], \"Data Domain\")\n",
    "dbutils.widgets.text(\"file_pattern\", \"\", \"File Pattern (optional)\")\n",
    "dbutils.widgets.dropdown(\"dedup_strategy\", \"keep_all\", \n",
    "                        [\"keep_all\", \"keep_latest\"], \n",
    "                        \"Deduplication Strategy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2abf0c39-7983-433f-87bd-94c40f26a307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Core Processing Functions\n",
    "\n",
    "def check_already_processing(domain: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if another job is currently processing files for this domain\n",
    "    Returns True if processing is already in progress\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Check for any runs in last 2 hours that haven't completed\n",
    "        in_progress = spark.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as count\n",
    "            FROM dev_bronze.bronze_audit.batch_processing_audit\n",
    "            WHERE domain = '{domain}'\n",
    "            AND start_time > current_timestamp() - INTERVAL 2 HOURS\n",
    "            AND end_time IS NULL\n",
    "        \"\"\").collect()[0].count\n",
    "        \n",
    "        return in_progress > 0\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def mark_files_as_processing(execution_id: str, files: list):\n",
    "    \"\"\"Mark files as being processed to prevent duplicate runs\"\"\"\n",
    "    \n",
    "    processing_schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"started_at\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    records = [(execution_id, f.name, \"PROCESSING\", datetime.now()) for f in files]\n",
    "    \n",
    "    processing_df = spark.createDataFrame(records, schema=processing_schema)\n",
    "    processing_df.write.mode(\"append\").saveAsTable(\"dev_bronze.bronze_audit.file_processing_locks\")\n",
    "\n",
    "def release_file_locks(execution_id: str):\n",
    "    \"\"\"Release file locks after processing completes\"\"\"\n",
    "    \n",
    "    try:\n",
    "        spark.sql(f\"\"\"\n",
    "            DELETE FROM dev_bronze.bronze_audit.file_processing_locks\n",
    "            WHERE execution_id = '{execution_id}'\n",
    "        \"\"\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def read_files_parallel(domain: str, file_pattern: str = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Read all unprocessed files in parallel using Spark\n",
    "    Returns: (DataFrame, list of file_info objects)\n",
    "    \"\"\"\n",
    "    \n",
    "    domain_folder = DOMAIN_FOLDER_MAP[domain]\n",
    "    incoming_path = f\"/mnt/{CONTAINER}/{domain_folder}/incoming\"\n",
    "    \n",
    "    # List all CSV files\n",
    "    all_files = dbutils.fs.ls(incoming_path)\n",
    "    csv_files = [f for f in all_files if f.name.endswith('.csv') and not f.isDir()]\n",
    "    \n",
    "    # Apply file pattern filter if specified\n",
    "    if file_pattern:\n",
    "        csv_files = [f for f in csv_files if file_pattern in f.name]\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        return None, []\n",
    "    \n",
    "    # Get already processed files from audit\n",
    "    try:\n",
    "        processed_files_df = spark.sql(\"\"\"\n",
    "            SELECT DISTINCT file_name \n",
    "            FROM dev_bronze.bronze_audit.file_processing_audit\n",
    "            WHERE status = 'SUCCESS'\n",
    "        \"\"\")\n",
    "        processed_files = set([row.file_name for row in processed_files_df.collect()])\n",
    "    except:\n",
    "        processed_files = set()\n",
    "    \n",
    "    # Filter to unprocessed files\n",
    "    files_to_process = [f for f in csv_files if f.name not in processed_files]\n",
    "    \n",
    "    if len(files_to_process) == 0:\n",
    "        return None, []\n",
    "    \n",
    "    # Load schema from registry\n",
    "    domain_config = get_domain_schema(domain)\n",
    "    schema = domain_config['schema']\n",
    "    \n",
    "    # Read all files in parallel with wildcard pattern\n",
    "    all_files_pattern = f\"{incoming_path}/*.csv\"\n",
    "    \n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .option(\"quote\", '\"') \\\n",
    "        .option(\"encoding\", \"utf-8\") \\\n",
    "        .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "        .option(\"mode\", \"PERMISSIVE\") \\\n",
    "        .option(\"pathGlobFilter\", \"*.csv\") \\\n",
    "        .load(all_files_pattern)\n",
    "    \n",
    "    # Add source file tracking using Unity Catalog compatible metadata\n",
    "    df = df.withColumn(\"_src_file_path\", col(\"_metadata.file_path\"))\n",
    "    df = df.withColumn(\"_src_file\", regexp_extract(col(\"_src_file_path\"), r\"([^/]+\\.csv)$\", 1))\n",
    "    \n",
    "    # Filter to only unprocessed files\n",
    "    unprocessed_names = [f.name for f in files_to_process]\n",
    "    df = df.filter(col(\"_src_file\").isin(unprocessed_names))\n",
    "    \n",
    "    return df, files_to_process\n",
    "\n",
    "def apply_deduplication(df: DataFrame, strategy: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Apply deduplication strategy\n",
    "    Returns: (DataFrame, count of duplicates removed)\n",
    "    \"\"\"\n",
    "    \n",
    "    if strategy == \"keep_all\":\n",
    "        return df, 0\n",
    "    \n",
    "    elif strategy == \"keep_latest\":\n",
    "        row_count_before = df.count()\n",
    "        \n",
    "        window = Window.partitionBy(\"load_number\") \\\n",
    "                       .orderBy(col(\"current_tender_timestamp\").desc())\n",
    "        \n",
    "        df = df.withColumn(\"_rn\", row_number().over(window)) \\\n",
    "               .filter(col(\"_rn\") == 1) \\\n",
    "               .drop(\"_rn\")\n",
    "        \n",
    "        row_count_after = df.count()\n",
    "        duplicates_removed = row_count_before - row_count_after\n",
    "        \n",
    "        return df, duplicates_removed\n",
    "    \n",
    "    else:\n",
    "        return df, 0\n",
    "\n",
    "def add_metadata(df: DataFrame, domain_config: dict, execution_id: str) -> DataFrame:\n",
    "    \"\"\"Add standard metadata columns and partition column\"\"\"\n",
    "    \n",
    "    df = df.withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    "    df = df.withColumn(\"_execution_id\", lit(execution_id))\n",
    "    df = df.withColumn(\"_schema_version\", lit(domain_config['version']))\n",
    "    \n",
    "    # Add partition column\n",
    "    partition_col = domain_config['partition_by']\n",
    "    if partition_col:\n",
    "        df = df.withColumn(partition_col, to_date(\"_ingestion_timestamp\"))\n",
    "    \n",
    "    # Trim string columns\n",
    "    string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "    for col_name in string_cols:\n",
    "        if not col_name.startswith('_'):\n",
    "            df = df.withColumn(col_name, trim(col(col_name)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def write_to_bronze(df: DataFrame, domain: str, domain_config: dict) -> dict:\n",
    "    \"\"\"Write DataFrame to bronze table with partitioning\"\"\"\n",
    "    \n",
    "    config = get_bronze_config(domain)\n",
    "    table_name = f\"dev_bronze.{config['schema']}.{config['table']}\"\n",
    "    partition_by = domain_config['partition_by']\n",
    "    \n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS dev_bronze.{config['schema']}\")\n",
    "    \n",
    "    table_exists = spark.catalog.tableExists(table_name)\n",
    "    \n",
    "    # Get execution_id before write for counting\n",
    "    execution_id_val = df.select('_execution_id').first()[0]\n",
    "    \n",
    "    start_write = time.time()\n",
    "    \n",
    "    if partition_by and not table_exists:\n",
    "        df.write.mode(\"append\") \\\n",
    "            .partitionBy(partition_by) \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "    else:\n",
    "        df.write.mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "    \n",
    "    # Count rows using table query (works on both serverless and classic)\n",
    "    row_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt \n",
    "        FROM {table_name} \n",
    "        WHERE _execution_id = '{execution_id_val}'\n",
    "    \"\"\").collect()[0].cnt\n",
    "    \n",
    "    write_duration = time.time() - start_write\n",
    "    \n",
    "    return {\n",
    "        'table_name': table_name,\n",
    "        'rows_written': row_count,\n",
    "        'write_duration': write_duration\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "369d511d-b8db-4d75-96d0-60d1c518db5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Audit and Cost Tracking Functions\n",
    "\n",
    "def create_audit_tables():\n",
    "    \"\"\"Create all audit and tracking tables if they don't exist\"\"\"\n",
    "    \n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS dev_bronze.bronze_audit\")\n",
    "    spark.sql(\"CREATE SCHEMA IF NOT EXISTS dev_bronze.cost_tracking\")\n",
    "    \n",
    "    # Batch processing audit\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.bronze_audit.batch_processing_audit (\n",
    "            execution_id STRING,\n",
    "            domain STRING,\n",
    "            file_count INT,\n",
    "            total_rows LONG,\n",
    "            start_time TIMESTAMP,\n",
    "            end_time TIMESTAMP,\n",
    "            duration_seconds DOUBLE,\n",
    "            user_name STRING,\n",
    "            status STRING,\n",
    "            processing_mode STRING,\n",
    "            schema_version INT,\n",
    "            duplicates_removed LONG\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Batch file details\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.bronze_audit.batch_file_details (\n",
    "            execution_id STRING,\n",
    "            file_name STRING,\n",
    "            file_size LONG,\n",
    "            file_modified_timestamp TIMESTAMP,\n",
    "            estimated_rows LONG,\n",
    "            status STRING\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Per-file processing audit (compatibility with Layer 2)\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.bronze_audit.file_processing_audit (\n",
    "            execution_id STRING,\n",
    "            file_name STRING,\n",
    "            file_size LONG,\n",
    "            file_modified_timestamp TIMESTAMP,\n",
    "            start_time TIMESTAMP,\n",
    "            end_time TIMESTAMP,\n",
    "            user_name STRING,\n",
    "            status STRING,\n",
    "            rows_processed LONG,\n",
    "            schema_info STRING,\n",
    "            file_properties STRING,\n",
    "            processing_details STRING\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.bronze_audit.performance_metrics (\n",
    "            execution_id STRING,\n",
    "            file_name STRING,\n",
    "            file_size_mb DOUBLE,\n",
    "            row_count LONG,\n",
    "            duration_seconds DOUBLE,\n",
    "            rows_per_second DOUBLE,\n",
    "            mb_per_second DOUBLE,\n",
    "            timestamp TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Notifications\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.bronze_audit.notifications (\n",
    "            timestamp TIMESTAMP,\n",
    "            severity STRING,\n",
    "            message STRING,\n",
    "            domain STRING,\n",
    "            execution_id STRING\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # Cost tracking\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.cost_tracking.ingestion_costs (\n",
    "            execution_id STRING,\n",
    "            file_name STRING,\n",
    "            domain STRING,\n",
    "            start_time TIMESTAMP,\n",
    "            end_time TIMESTAMP,\n",
    "            duration_seconds DOUBLE,\n",
    "            rows_processed LONG,\n",
    "            file_size_mb DOUBLE,\n",
    "            cluster_type STRING,\n",
    "            num_workers INT,\n",
    "            compute_cost_usd DOUBLE,\n",
    "            storage_cost_usd DOUBLE,\n",
    "            data_transfer_cost_usd DOUBLE,\n",
    "            total_cost_usd DOUBLE,\n",
    "            cost_per_row DOUBLE,\n",
    "            cost_per_mb DOUBLE,\n",
    "            driver_node_type STRING,\n",
    "            worker_node_type STRING,\n",
    "            recorded_at TIMESTAMP,\n",
    "            processing_details STRING\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "    \n",
    "    # File processing locks (for idempotency)\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS dev_bronze.bronze_audit.file_processing_locks (\n",
    "            execution_id STRING,\n",
    "            file_name STRING,\n",
    "            status STRING,\n",
    "            started_at TIMESTAMP\n",
    "        ) USING DELTA\n",
    "    \"\"\")\n",
    "\n",
    "def get_cluster_info():\n",
    "    \"\"\"Get cluster configuration information - identifies all Databricks compute types\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get cluster context\n",
    "        cluster_id = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")\n",
    "        cluster_source = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterSource\", \"\")\n",
    "        \n",
    "        # Detect compute type\n",
    "        is_serverless = \"serverless\" in cluster_id.lower() or cluster_id == \"\"\n",
    "        is_job_compute = \"job-\" in cluster_id.lower() or cluster_source == \"JOB\"\n",
    "        is_all_purpose = cluster_source == \"UI\" or cluster_source == \"INTERACTIVE_NOTEBOOK\"\n",
    "        is_sql_warehouse = \"sql\" in cluster_id.lower() or cluster_source == \"SQL\"\n",
    "        is_dlt_pipeline = \"dlt\" in cluster_id.lower() or cluster_source == \"DLT\"\n",
    "        \n",
    "        # Determine compute type string\n",
    "        if is_serverless:\n",
    "            compute_type = \"Serverless\"\n",
    "            cluster_display = \"Serverless Compute\"\n",
    "        elif is_sql_warehouse:\n",
    "            compute_type = \"SQL Warehouse\"\n",
    "            cluster_display = f\"SQL Warehouse ({cluster_id[:8]}...)\"\n",
    "        elif is_dlt_pipeline:\n",
    "            compute_type = \"DLT Pipeline\"\n",
    "            cluster_display = f\"DLT Pipeline ({cluster_id[:8]}...)\"\n",
    "        elif is_job_compute:\n",
    "            compute_type = \"Job Compute\"\n",
    "            cluster_display = f\"Job Cluster ({cluster_id[:8]}...)\"\n",
    "        elif is_all_purpose:\n",
    "            compute_type = \"All-Purpose Compute\"\n",
    "            cluster_display = f\"All-Purpose ({cluster_id[:8]}...)\"\n",
    "        else:\n",
    "            compute_type = \"Classic Cluster\"\n",
    "            cluster_display = f\"Classic ({cluster_id[:8]}...)\"\n",
    "        \n",
    "        # Get worker count (0 for serverless)\n",
    "        try:\n",
    "            num_workers = int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkers\", \"0\"))\n",
    "        except:\n",
    "            num_workers = 0 if is_serverless else 2\n",
    "        \n",
    "        # Get node types\n",
    "        try:\n",
    "            driver_node = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\", \"Standard_DS3_v2\")\n",
    "            worker_node = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkerNodeType\", driver_node)\n",
    "        except:\n",
    "            driver_node = \"Serverless\" if is_serverless else \"Standard_DS3_v2\"\n",
    "            worker_node = driver_node\n",
    "        \n",
    "        return {\n",
    "            \"cluster_id\": cluster_id if cluster_id else \"serverless\",\n",
    "            \"compute_type\": compute_type,\n",
    "            \"cluster_display\": cluster_display,\n",
    "            \"driver_node_type\": driver_node,\n",
    "            \"worker_node_type\": worker_node,\n",
    "            \"num_workers\": num_workers,\n",
    "            \"is_serverless\": is_serverless,\n",
    "            \"is_job_compute\": is_job_compute,\n",
    "            \"is_all_purpose\": is_all_purpose,\n",
    "            \"is_sql_warehouse\": is_sql_warehouse,\n",
    "            \"is_dlt_pipeline\": is_dlt_pipeline\n",
    "        }\n",
    "    except:\n",
    "        # Fallback for environments where cluster info isn't available\n",
    "        return {\n",
    "            \"cluster_id\": \"unknown\",\n",
    "            \"compute_type\": \"Unknown\",\n",
    "            \"cluster_display\": \"Unknown Compute\",\n",
    "            \"driver_node_type\": \"Unknown\",\n",
    "            \"worker_node_type\": \"Unknown\",\n",
    "            \"num_workers\": 0,\n",
    "            \"is_serverless\": False,\n",
    "            \"is_job_compute\": False,\n",
    "            \"is_all_purpose\": False,\n",
    "            \"is_sql_warehouse\": False,\n",
    "            \"is_dlt_pipeline\": False\n",
    "        }\n",
    "\n",
    "def calculate_cluster_cost(cluster_info: dict, duration_hours: float) -> dict:\n",
    "    \"\"\"Calculate cluster cost based on compute type and usage\"\"\"\n",
    "    \n",
    "    compute_type = cluster_info.get(\"compute_type\", \"Unknown\")\n",
    "    \n",
    "    # Serverless pricing (DBU-based only, no VM costs)\n",
    "    if cluster_info.get(\"is_serverless\", False):\n",
    "        dbu_rate = COST_CONFIG[\"dbu_costs\"][\"serverless\"]\n",
    "        estimated_dbus = duration_hours * 5  # Rough estimate: 5 DBUs per hour\n",
    "        total_cost = estimated_dbus * dbu_rate\n",
    "        return {\n",
    "            \"compute_cost\": total_cost,\n",
    "            \"vm_cost\": 0,\n",
    "            \"dbu_cost\": total_cost,\n",
    "            \"dbus_used\": estimated_dbus,\n",
    "            \"pricing_model\": \"Serverless (DBU only)\"\n",
    "        }\n",
    "    \n",
    "    # SQL Warehouse pricing\n",
    "    elif cluster_info.get(\"is_sql_warehouse\", False):\n",
    "        dbu_rate = COST_CONFIG[\"dbu_costs\"][\"sql_warehouse\"]\n",
    "        estimated_dbus = duration_hours * 8\n",
    "        total_cost = estimated_dbus * dbu_rate\n",
    "        return {\n",
    "            \"compute_cost\": total_cost,\n",
    "            \"vm_cost\": 0,\n",
    "            \"dbu_cost\": total_cost,\n",
    "            \"dbus_used\": estimated_dbus,\n",
    "            \"pricing_model\": \"SQL Warehouse (DBU only)\"\n",
    "        }\n",
    "    \n",
    "    # DLT Pipeline pricing\n",
    "    elif cluster_info.get(\"is_dlt_pipeline\", False):\n",
    "        dbu_rate = COST_CONFIG[\"dbu_costs\"][\"dlt_pipeline\"]\n",
    "        estimated_dbus = duration_hours * 6\n",
    "        total_cost = estimated_dbus * dbu_rate\n",
    "        return {\n",
    "            \"compute_cost\": total_cost,\n",
    "            \"vm_cost\": 0,\n",
    "            \"dbu_cost\": total_cost,\n",
    "            \"dbus_used\": estimated_dbus,\n",
    "            \"pricing_model\": \"DLT Pipeline (DBU only)\"\n",
    "        }\n",
    "    \n",
    "    # Job Compute pricing (VM + DBU)\n",
    "    elif cluster_info.get(\"is_job_compute\", False):\n",
    "        driver_node = cluster_info[\"driver_node_type\"].lower().replace(\"_\", \"\")\n",
    "        worker_node = cluster_info[\"worker_node_type\"].lower().replace(\"_\", \"\")\n",
    "        \n",
    "        driver_vm_cost = COST_CONFIG[\"vm_costs\"].get(driver_node, COST_CONFIG[\"vm_costs\"][\"unknown\"])\n",
    "        worker_vm_cost = COST_CONFIG[\"vm_costs\"].get(worker_node, COST_CONFIG[\"vm_costs\"][\"unknown\"])\n",
    "        \n",
    "        total_vm_cost = (driver_vm_cost + (worker_vm_cost * cluster_info[\"num_workers\"])) * duration_hours\n",
    "        \n",
    "        dbu_rate = COST_CONFIG[\"dbu_costs\"][\"jobs_compute\"]\n",
    "        estimated_dbus = (1 + cluster_info[\"num_workers\"]) * duration_hours * 2\n",
    "        total_dbu_cost = estimated_dbus * dbu_rate\n",
    "        \n",
    "        return {\n",
    "            \"compute_cost\": total_vm_cost + total_dbu_cost,\n",
    "            \"vm_cost\": total_vm_cost,\n",
    "            \"dbu_cost\": total_dbu_cost,\n",
    "            \"dbus_used\": estimated_dbus,\n",
    "            \"pricing_model\": \"Job Compute (VM + DBU)\"\n",
    "        }\n",
    "    \n",
    "    # All-Purpose Compute pricing (VM + higher DBU rate)\n",
    "    elif cluster_info.get(\"is_all_purpose\", False):\n",
    "        driver_node = cluster_info[\"driver_node_type\"].lower().replace(\"_\", \"\")\n",
    "        worker_node = cluster_info[\"worker_node_type\"].lower().replace(\"_\", \"\")\n",
    "        \n",
    "        driver_vm_cost = COST_CONFIG[\"vm_costs\"].get(driver_node, COST_CONFIG[\"vm_costs\"][\"unknown\"])\n",
    "        worker_vm_cost = COST_CONFIG[\"vm_costs\"].get(worker_node, COST_CONFIG[\"vm_costs\"][\"unknown\"])\n",
    "        \n",
    "        total_vm_cost = (driver_vm_cost + (worker_vm_cost * cluster_info[\"num_workers\"])) * duration_hours\n",
    "        \n",
    "        dbu_rate = COST_CONFIG[\"dbu_costs\"][\"all_purpose\"]\n",
    "        estimated_dbus = (1 + cluster_info[\"num_workers\"]) * duration_hours * 2\n",
    "        total_dbu_cost = estimated_dbus * dbu_rate\n",
    "        \n",
    "        return {\n",
    "            \"compute_cost\": total_vm_cost + total_dbu_cost,\n",
    "            \"vm_cost\": total_vm_cost,\n",
    "            \"dbu_cost\": total_dbu_cost,\n",
    "            \"dbus_used\": estimated_dbus,\n",
    "            \"pricing_model\": \"All-Purpose (VM + DBU Premium)\"\n",
    "        }\n",
    "    \n",
    "    # Classic/Unknown (fallback)\n",
    "    else:\n",
    "        driver_node = cluster_info[\"driver_node_type\"].lower().replace(\"_\", \"\")\n",
    "        worker_node = cluster_info[\"worker_node_type\"].lower().replace(\"_\", \"\")\n",
    "        \n",
    "        driver_vm_cost = COST_CONFIG[\"vm_costs\"].get(driver_node, COST_CONFIG[\"vm_costs\"][\"unknown\"])\n",
    "        worker_vm_cost = COST_CONFIG[\"vm_costs\"].get(worker_node, COST_CONFIG[\"vm_costs\"][\"unknown\"])\n",
    "        \n",
    "        total_vm_cost = (driver_vm_cost + (worker_vm_cost * cluster_info.get(\"num_workers\", 0))) * duration_hours\n",
    "        \n",
    "        dbu_rate = COST_CONFIG[\"dbu_costs\"][\"classic\"]\n",
    "        estimated_dbus = (1 + cluster_info.get(\"num_workers\", 0)) * duration_hours * 2\n",
    "        total_dbu_cost = estimated_dbus * dbu_rate\n",
    "        \n",
    "        return {\n",
    "            \"compute_cost\": total_vm_cost + total_dbu_cost,\n",
    "            \"vm_cost\": total_vm_cost,\n",
    "            \"dbu_cost\": total_dbu_cost,\n",
    "            \"dbus_used\": estimated_dbus,\n",
    "            \"pricing_model\": \"Classic (VM + DBU)\"\n",
    "        }\n",
    "\n",
    "def log_batch_audit(execution_id: str, domain: str, files: list, total_rows: int, \n",
    "                    duration: float, schema_version: int, duplicates_removed: int, \n",
    "                    status: str = \"SUCCESS\"):\n",
    "    \"\"\"Log batch-level audit record - matches existing table schema\"\"\"\n",
    "    \n",
    "    try:\n",
    "        user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    except:\n",
    "        user = \"system\"\n",
    "    \n",
    "    start_time = datetime.now() - timedelta(seconds=duration)\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    # Match existing table schema (no duplicates_removed column)\n",
    "    batch_schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"file_count\", IntegerType(), True),\n",
    "        StructField(\"total_rows\", LongType(), True),\n",
    "        StructField(\"start_time\", TimestampType(), True),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"duration_seconds\", DoubleType(), True),\n",
    "        StructField(\"user_name\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"processing_mode\", StringType(), True),\n",
    "        StructField(\"schema_version\", IntegerType(), True)\n",
    "    ])\n",
    "    \n",
    "    batch_df = spark.createDataFrame([(\n",
    "        execution_id, domain, len(files), total_rows, start_time, end_time,\n",
    "        duration, user, status, \"spark_parallel\", schema_version\n",
    "    )], schema=batch_schema)\n",
    "    \n",
    "    batch_df.write.mode(\"append\").saveAsTable(\"dev_bronze.bronze_audit.batch_processing_audit\")\n",
    "    \n",
    "    # Log file details\n",
    "    file_details = []\n",
    "    for f in files:\n",
    "        file_details.append((\n",
    "            execution_id,\n",
    "            f.name,\n",
    "            f.size,\n",
    "            datetime.fromtimestamp(f.modificationTime / 1000),\n",
    "            total_rows // len(files),\n",
    "            status\n",
    "        ))\n",
    "    \n",
    "    detail_schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"file_size\", LongType(), True),\n",
    "        StructField(\"file_modified_timestamp\", TimestampType(), True),\n",
    "        StructField(\"estimated_rows\", LongType(), True),\n",
    "        StructField(\"status\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    detail_df = spark.createDataFrame(file_details, schema=detail_schema)\n",
    "    detail_df.write.mode(\"append\").saveAsTable(\"dev_bronze.bronze_audit.batch_file_details\")\n",
    "\n",
    "def log_per_file_audit(execution_id: str, files: list, total_rows: int, \n",
    "                       duration: float, schema_version: int, status: str = \"SUCCESS\"):\n",
    "    \"\"\"Log per-file audit records for compatibility with existing queries\"\"\"\n",
    "    \n",
    "    try:\n",
    "        user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "    except:\n",
    "        user = \"system\"\n",
    "    \n",
    "    start_time = datetime.now() - timedelta(seconds=duration)\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    file_records = []\n",
    "    rows_per_file = total_rows // len(files) if len(files) > 0 else 0\n",
    "    \n",
    "    for f in files:\n",
    "        schema_info = json.dumps({'schema_version': schema_version})\n",
    "        file_props = json.dumps({'encoding': 'utf-8', 'delimiter': ','})\n",
    "        processing_details = json.dumps({\n",
    "            'processing_mode': 'spark_parallel',\n",
    "            'schema_source': 'registry',\n",
    "            'optimized': True,\n",
    "            'layer': 'Layer 3'\n",
    "        })\n",
    "        \n",
    "        file_records.append((\n",
    "            execution_id,\n",
    "            f.name,\n",
    "            f.size,\n",
    "            datetime.fromtimestamp(f.modificationTime / 1000),\n",
    "            start_time,\n",
    "            end_time,\n",
    "            user,\n",
    "            status,\n",
    "            rows_per_file,\n",
    "            schema_info,\n",
    "            file_props,\n",
    "            processing_details\n",
    "        ))\n",
    "    \n",
    "    audit_schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"file_size\", LongType(), True),\n",
    "        StructField(\"file_modified_timestamp\", TimestampType(), True),\n",
    "        StructField(\"start_time\", TimestampType(), True),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"user_name\", StringType(), True),\n",
    "        StructField(\"status\", StringType(), True),\n",
    "        StructField(\"rows_processed\", LongType(), True),\n",
    "        StructField(\"schema_info\", StringType(), True),\n",
    "        StructField(\"file_properties\", StringType(), True),\n",
    "        StructField(\"processing_details\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    audit_df = spark.createDataFrame(file_records, schema=audit_schema)\n",
    "    audit_df.write.mode(\"append\").saveAsTable(\"dev_bronze.bronze_audit.file_processing_audit\")\n",
    "\n",
    "def log_performance_metrics(execution_id: str, files: list, total_rows: int, duration: float):\n",
    "    \"\"\"Log performance metrics per file\"\"\"\n",
    "    \n",
    "    metrics_records = []\n",
    "    rows_per_file = total_rows // len(files) if len(files) > 0 else 0\n",
    "    duration_per_file = duration / len(files) if len(files) > 0 else duration\n",
    "    \n",
    "    for f in files:\n",
    "        file_size_mb = f.size / (1024 * 1024)\n",
    "        rows_per_sec = rows_per_file / duration_per_file if duration_per_file > 0 else 0\n",
    "        mb_per_sec = file_size_mb / duration_per_file if duration_per_file > 0 else 0\n",
    "        \n",
    "        metrics_records.append((\n",
    "            execution_id,\n",
    "            f.name,\n",
    "            file_size_mb,\n",
    "            rows_per_file,\n",
    "            duration_per_file,\n",
    "            rows_per_sec,\n",
    "            mb_per_sec,\n",
    "            datetime.now()\n",
    "        ))\n",
    "    \n",
    "    metrics_schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"file_size_mb\", DoubleType(), True),\n",
    "        StructField(\"row_count\", LongType(), True),\n",
    "        StructField(\"duration_seconds\", DoubleType(), True),\n",
    "        StructField(\"rows_per_second\", DoubleType(), True),\n",
    "        StructField(\"mb_per_second\", DoubleType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    metrics_df = spark.createDataFrame(metrics_records, schema=metrics_schema)\n",
    "    metrics_df.write.mode(\"append\").saveAsTable(\"dev_bronze.bronze_audit.performance_metrics\")\n",
    "\n",
    "def log_cost_tracking(execution_id: str, domain: str, files: list, total_rows: int, duration: float):\n",
    "    \"\"\"Log cost tracking per file - matches existing table schema\"\"\"\n",
    "    \n",
    "    cluster_info = get_cluster_info()\n",
    "    duration_hours = duration / 3600\n",
    "    cost_breakdown = calculate_cluster_cost(cluster_info, duration_hours)\n",
    "    \n",
    "    cost_records = []\n",
    "    rows_per_file = total_rows // len(files) if len(files) > 0 else 0\n",
    "    duration_per_file = duration / len(files) if len(files) > 0 else duration\n",
    "    cost_per_file = cost_breakdown[\"compute_cost\"] / len(files) if len(files) > 0 else 0\n",
    "    \n",
    "    for f in files:\n",
    "        file_size_mb = f.size / (1024 * 1024)\n",
    "        storage_cost = file_size_mb / 1024 * COST_CONFIG[\"storage_costs\"][\"hot_tier\"] / 30\n",
    "        total_cost = cost_per_file + storage_cost\n",
    "        cost_per_row = total_cost / rows_per_file if rows_per_file > 0 else 0\n",
    "        cost_per_mb = total_cost / file_size_mb if file_size_mb > 0 else 0\n",
    "        \n",
    "        start_time = datetime.now() - timedelta(seconds=duration)\n",
    "        \n",
    "        # Match existing table schema exactly\n",
    "        cost_records.append((\n",
    "            execution_id,              # execution_id\n",
    "            f.name,                    # file_name\n",
    "            domain,                    # domain\n",
    "            start_time,                # start_time\n",
    "            datetime.now(),            # end_time\n",
    "            duration_per_file,         # duration_seconds\n",
    "            rows_per_file,             # rows_processed (bigint)\n",
    "            file_size_mb,              # file_size_mb\n",
    "            cluster_info[\"cluster_id\"], # cluster_id\n",
    "            cluster_info[\"num_workers\"], # num_workers\n",
    "            cost_per_file,             # compute_cost_usd\n",
    "            total_cost,                # total_cost_usd\n",
    "            cost_per_row,              # cost_per_row\n",
    "            cost_per_mb,               # cost_per_mb\n",
    "            datetime.now(),            # created_timestamp\n",
    "            storage_cost,              # storage_cost_usd\n",
    "            0.0,                       # data_transfer_cost_usd\n",
    "            cluster_info[\"driver_node_type\"], # driver_node_type\n",
    "            cluster_info[\"worker_node_type\"]  # worker_node_type\n",
    "        ))\n",
    "    \n",
    "    # Match existing table schema column order\n",
    "    cost_schema = StructType([\n",
    "        StructField(\"execution_id\", StringType(), True),\n",
    "        StructField(\"file_name\", StringType(), True),\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"start_time\", TimestampType(), True),\n",
    "        StructField(\"end_time\", TimestampType(), True),\n",
    "        StructField(\"duration_seconds\", DoubleType(), True),\n",
    "        StructField(\"rows_processed\", LongType(), True),\n",
    "        StructField(\"file_size_mb\", DoubleType(), True),\n",
    "        StructField(\"cluster_id\", StringType(), True),\n",
    "        StructField(\"num_workers\", IntegerType(), True),\n",
    "        StructField(\"compute_cost_usd\", DoubleType(), True),\n",
    "        StructField(\"total_cost_usd\", DoubleType(), True),\n",
    "        StructField(\"cost_per_row\", DoubleType(), True),\n",
    "        StructField(\"cost_per_mb\", DoubleType(), True),\n",
    "        StructField(\"created_timestamp\", TimestampType(), True),\n",
    "        StructField(\"storage_cost_usd\", DoubleType(), True),\n",
    "        StructField(\"data_transfer_cost_usd\", DoubleType(), True),\n",
    "        StructField(\"driver_node_type\", StringType(), True),\n",
    "        StructField(\"worker_node_type\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    cost_df = spark.createDataFrame(cost_records, schema=cost_schema)\n",
    "    cost_df.write.mode(\"append\").saveAsTable(\"dev_bronze.cost_tracking.ingestion_costs\")\n",
    "\n",
    "def send_notification(message: str, severity: str, domain: str, execution_id: str):\n",
    "    \"\"\"Log notification for issues or alerts\"\"\"\n",
    "    \n",
    "    notif_schema = StructType([\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"severity\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"execution_id\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    notif_df = spark.createDataFrame([(\n",
    "        datetime.now(), severity, message, domain, execution_id\n",
    "    )], schema=notif_schema)\n",
    "    \n",
    "    notif_df.write.mode(\"append\").saveAsTable(\"dev_bronze.bronze_audit.notifications\")\n",
    "\n",
    "def archive_files_batch(files: list, domain: str):\n",
    "    \"\"\"Archive all processed files to processed folder\"\"\"\n",
    "    \n",
    "    date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "    domain_folder = DOMAIN_FOLDER_MAP[domain]\n",
    "    dest_base = f\"/mnt/{CONTAINER}/{domain_folder}/processed/{date_str}\"\n",
    "    \n",
    "    dbutils.fs.mkdirs(dest_base)\n",
    "    \n",
    "    for file_info in files:\n",
    "        try:\n",
    "            source = f\"/mnt/{CONTAINER}/{domain_folder}/incoming/{file_info.name}\"\n",
    "            dest = f\"{dest_base}/{file_info.name}\"\n",
    "            dbutils.fs.mv(source, dest)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9656bd-f1cc-4dfb-aec8-acee97d973b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main Processing Function\n",
    "\n",
    "def process_batch_parallel(domain: str, file_pattern: str = None, dedup_strategy: str = \"keep_all\") -> dict:\n",
    "    \"\"\"\n",
    "    Main batch processing function using Spark parallel processing\n",
    "    \n",
    "    Args:\n",
    "        domain: Data domain (e.g., 'Load Detail')\n",
    "        file_pattern: Optional filter for file names\n",
    "        dedup_strategy: 'keep_all' or 'keep_latest'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with processing results\n",
    "    \"\"\"\n",
    "    \n",
    "    execution_id = str(uuid.uuid4())\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create audit tables\n",
    "        create_audit_tables()\n",
    "        \n",
    "        # Check if another job is already processing\n",
    "        if check_already_processing(domain):\n",
    "            return {\n",
    "                'success': False,\n",
    "                'files_processed': 0,\n",
    "                'rows_processed': 0,\n",
    "                'duration': 0,\n",
    "                'table_name': None,\n",
    "                'message': 'Another job is already processing this domain',\n",
    "                'execution_id': execution_id\n",
    "            }\n",
    "        \n",
    "        # Read all files in parallel\n",
    "        df, files_to_process = read_files_parallel(domain, file_pattern)\n",
    "        \n",
    "        if df is None or len(files_to_process) == 0:\n",
    "            return {\n",
    "                'success': True,\n",
    "                'files_processed': 0,\n",
    "                'rows_processed': 0,\n",
    "                'duration': 0,\n",
    "                'table_name': None,\n",
    "                'message': 'No files to process',\n",
    "                'execution_id': execution_id\n",
    "            }\n",
    "        \n",
    "        # Mark files as being processed (idempotency lock)\n",
    "        mark_files_as_processing(execution_id, files_to_process)\n",
    "        \n",
    "        # Apply deduplication\n",
    "        df, duplicates_removed = apply_deduplication(df, dedup_strategy)\n",
    "        \n",
    "        # Load domain configuration and add metadata\n",
    "        domain_config = get_domain_schema(domain)\n",
    "        df = add_metadata(df, domain_config, execution_id)\n",
    "        \n",
    "        # Write to bronze table\n",
    "        write_result = write_to_bronze(df, domain, domain_config)\n",
    "        \n",
    "        # Archive files\n",
    "        archive_files_batch(files_to_process, domain)\n",
    "        \n",
    "        # Release processing locks\n",
    "        release_file_locks(execution_id)\n",
    "        \n",
    "        # Calculate totals\n",
    "        overall_duration = time.time() - overall_start\n",
    "        \n",
    "        # Log all audit tables\n",
    "        try:\n",
    "            log_batch_audit(execution_id, domain, files_to_process, \n",
    "                           write_result['rows_written'], overall_duration, \n",
    "                           domain_config['version'], duplicates_removed, \"SUCCESS\")\n",
    "        except Exception as e:\n",
    "            send_notification(f\"Failed to log batch audit: {str(e)}\", \"ERROR\", domain, execution_id)\n",
    "        \n",
    "        try:\n",
    "            log_per_file_audit(execution_id, files_to_process, \n",
    "                              write_result['rows_written'], overall_duration, \n",
    "                              domain_config['version'], \"SUCCESS\")\n",
    "        except Exception as e:\n",
    "            send_notification(f\"Failed to log per-file audit: {str(e)}\", \"ERROR\", domain, execution_id)\n",
    "        \n",
    "        try:\n",
    "            log_performance_metrics(execution_id, files_to_process, \n",
    "                                   write_result['rows_written'], overall_duration)\n",
    "        except Exception as e:\n",
    "            send_notification(f\"Failed to log performance metrics: {str(e)}\", \"ERROR\", domain, execution_id)\n",
    "        \n",
    "        try:\n",
    "            log_cost_tracking(execution_id, domain, files_to_process, \n",
    "                             write_result['rows_written'], overall_duration)\n",
    "        except Exception as e:\n",
    "            send_notification(f\"Failed to log cost tracking: {str(e)}\", \"ERROR\", domain, execution_id)\n",
    "        \n",
    "        # Check for schema changes\n",
    "        try:\n",
    "            existing_df = spark.table(write_result['table_name']).limit(0)\n",
    "            existing_cols = set(existing_df.columns)\n",
    "            new_cols = set(df.columns)\n",
    "            \n",
    "            added_cols = new_cols - existing_cols\n",
    "            if added_cols:\n",
    "                send_notification(\n",
    "                    f\"New columns detected in {domain}: {sorted(added_cols)}\",\n",
    "                    \"INFO\",\n",
    "                    domain,\n",
    "                    execution_id\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'files_processed': len(files_to_process),\n",
    "            'rows_processed': write_result['rows_written'],\n",
    "            'duration': overall_duration,\n",
    "            'table_name': write_result['table_name'],\n",
    "            'execution_id': execution_id\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        overall_duration = time.time() - overall_start\n",
    "        \n",
    "        # Release locks on failure\n",
    "        release_file_locks(execution_id)\n",
    "        \n",
    "        send_notification(\n",
    "            f\"Batch processing failed: {str(e)}\",\n",
    "            \"ERROR\",\n",
    "            domain,\n",
    "            execution_id\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'files_processed': 0,\n",
    "            'rows_processed': 0,\n",
    "            'duration': overall_duration,\n",
    "            'table_name': None,\n",
    "            'error': str(e),\n",
    "            'execution_id': execution_id\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585d7da5-b0de-485e-aa76-5eeb630e0c2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Execute Processing\n",
    "\n",
    "domain = dbutils.widgets.get(\"domain\")\n",
    "file_pattern = dbutils.widgets.get(\"file_pattern\")\n",
    "dedup_strategy = dbutils.widgets.get(\"dedup_strategy\")\n",
    "\n",
    "if not file_pattern or file_pattern.strip() == \"\":\n",
    "    file_pattern = None\n",
    "\n",
    "results = process_batch_parallel(domain, file_pattern, dedup_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddeb9196-f1c5-4334-a634-2f296124d483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Validation and Exit Code\n",
    "\n",
    "if not results['success']:\n",
    "    # Log failure\n",
    "    error_msg = results.get('error', results.get('message', 'Unknown error'))\n",
    "    \n",
    "    # Send critical notification\n",
    "    try:\n",
    "        send_notification(\n",
    "            f\"Job failed: {error_msg}\",\n",
    "            \"ERROR\",\n",
    "            domain,\n",
    "            results['execution_id']\n",
    "        )\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Exit with failure code\n",
    "    dbutils.notebook.exit(f\"FAILED: {error_msg}\")\n",
    "\n",
    "elif results['files_processed'] == 0:\n",
    "    # No files to process - this is OK, exit successfully\n",
    "    dbutils.notebook.exit(f\"SUCCESS: No files to process\")\n",
    "\n",
    "else:\n",
    "    # Successful processing - validate results\n",
    "    files_processed = results['files_processed']\n",
    "    rows_processed = results['rows_processed']\n",
    "    duration = results['duration']\n",
    "    \n",
    "    # Basic validation checks\n",
    "    if rows_processed == 0:\n",
    "        error_msg = f\"Processed {files_processed} files but got 0 rows - possible data issue\"\n",
    "        send_notification(error_msg, \"WARNING\", domain, results['execution_id'])\n",
    "        dbutils.notebook.exit(f\"WARNING: {error_msg}\")\n",
    "    \n",
    "    if duration > 600:  # More than 10 minutes\n",
    "        warning_msg = f\"Processing took {duration:.0f} seconds - consider performance optimization\"\n",
    "        send_notification(warning_msg, \"WARNING\", domain, results['execution_id'])\n",
    "    \n",
    "    # Success - exit with summary\n",
    "    summary = f\"SUCCESS: Processed {files_processed} files, {rows_processed:,} rows in {duration:.1f}s\"\n",
    "    dbutils.notebook.exit(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91a4311-d260-4099-bf89-c2f02a7e275e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Z-Order Optimization\n",
    "\n",
    "if results['success'] and results['table_name']:\n",
    "    try:\n",
    "        domain_config = get_domain_schema(domain)\n",
    "        zorder_cols = domain_config['zorder_by']\n",
    "        \n",
    "        if zorder_cols:\n",
    "            zorder_sql = f\"\"\"\n",
    "                OPTIMIZE {results['table_name']}\n",
    "                ZORDER BY ({', '.join(zorder_cols)})\n",
    "            \"\"\"\n",
    "            spark.sql(zorder_sql)\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "nb_bronze_ingestion",
   "widgets": {
    "dedup_strategy": {
     "currentValue": "keep_all",
     "nuid": "3bf29a1f-8d4e-43ae-a5fb-df7fb1f721d3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "keep_all",
      "label": "Deduplication Strategy",
      "name": "dedup_strategy",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "keep_all",
        "keep_latest"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "keep_all",
      "label": "Deduplication Strategy",
      "name": "dedup_strategy",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "keep_all",
        "keep_latest"
       ]
      }
     }
    },
    "domain": {
     "currentValue": "Load Detail",
     "nuid": "f9b76ed2-9622-4b7d-b969-912aa32106d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Load Detail",
      "label": "Data Domain",
      "name": "domain",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Load Detail"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Load Detail",
      "label": "Data Domain",
      "name": "domain",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "Load Detail"
       ]
      }
     }
    },
    "file_pattern": {
     "currentValue": "",
     "nuid": "06aa2afc-0a49-421c-8025-4e7e3b81d9ff",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "File Pattern (optional)",
      "name": "file_pattern",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "File Pattern (optional)",
      "name": "file_pattern",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}