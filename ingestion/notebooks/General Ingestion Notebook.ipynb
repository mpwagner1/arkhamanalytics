{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a5d8415-4003-4e57-ac76-e65d718441c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, List\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import uuid\n",
    "import io\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31e9e7be-e729-4266-bcf2-b83bca88fdd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for file processing\"\"\"\n",
    "    container_name: str\n",
    "    file_pattern: str\n",
    "    encoding: str = \"\"\n",
    "    delimiter: str = \"\"\n",
    "    quotechar: str = \"\"\n",
    "    escapechar: str = \"\"\n",
    "    skip_lines: int = 0\n",
    "    audit_table: str = \"file_processing_audit\"\n",
    "    sheet_name: str = \"\"\n",
    "    excel_starting_cell: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30a9dddc-ebe2-472f-b511-debc27c1c2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    %pip install openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a576927-9e0a-41da-84a7-f3f0834d64e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    class WidgetManager:\n",
    "        \"\"\"Manages the lifecycle of Databricks widgets\"\"\"\n",
    "\n",
    "        # Widget definitions by domain\n",
    "        WIDGET_GROUPS = {\n",
    "            \"Accounts Payable\": [\n",
    "                \"INVOICE_NUMBER\",\n",
    "                \"INVOICE_LINE_NUM\",\n",
    "                \"INVOICE_DATE\",\n",
    "                \"SUPPLIER_MASTER_ID\",\n",
    "                \"SUPPLIER_NAME\",\n",
    "                \"INVOICE_NET_AMOUNT\",\n",
    "                \"INVOICE_NET_AMOUNT_USD\",\n",
    "                \"INVOICE_QTY\",\n",
    "                \"RECEIVED_DATE\",\n",
    "                \"confirm_mapping\"\n",
    "            ],\n",
    "            \"Purchase Orders\": [\n",
    "                \"PURCHASE_ORDER_NUM\",\n",
    "                \"PURCHASE_ORDER_LINE_NUM\",\n",
    "                \"PURCHASE_ORDER_DATE\",\n",
    "                \"SUPPLIER_MASTER_ID\",\n",
    "                \"SUPPLIER_NAME\",\n",
    "                \"TOTAL_SPEND_LOCAL\",\n",
    "                \"TOTAL_SPEND_USD\",\n",
    "                \"RECEIVED_DATE\",\n",
    "                \"confirm_mapping\"\n",
    "            ],\n",
    "            \"Item Master\": [\n",
    "                \"PART_NUMBER\",\n",
    "                \"PART_NAME\",\n",
    "                \"PART_DESCRIPTION\",\n",
    "                \"confirm_mapping\"\n",
    "            ],\n",
    "            \"Supplier Master\": [\n",
    "                \"SUPPLIER_ID\",\n",
    "                \"SUPPLIER_NAME\",\n",
    "                \"confirm_mapping\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        @staticmethod\n",
    "        def check_widget_exists(widget_name: str) -> bool:\n",
    "            \"\"\"Safely check if a widget exists\"\"\"\n",
    "            try:\n",
    "                dbutils.widgets.get(widget_name)\n",
    "                return True\n",
    "            except Exception:\n",
    "                return False\n",
    "\n",
    "        @staticmethod\n",
    "        def cleanup_widgets(domain: str = None):\n",
    "            \"\"\"Cleans up widgets based on domain\"\"\"\n",
    "            if domain and domain in WidgetManager.WIDGET_GROUPS:\n",
    "                widgets_to_remove = WidgetManager.WIDGET_GROUPS[domain]\n",
    "                print(f\"Cleaning up widgets for domain: {domain}\")\n",
    "            else:\n",
    "                widgets_to_remove = []\n",
    "                for group in WidgetManager.WIDGET_GROUPS.values():\n",
    "                    widgets_to_remove.extend(group)\n",
    "                print(\"Cleaning up all mapping widgets...\")\n",
    "\n",
    "            removed_count = 0\n",
    "            for widget in widgets_to_remove:\n",
    "                if WidgetManager.check_widget_exists(widget):\n",
    "                    try:\n",
    "                        dbutils.widgets.remove(widget)\n",
    "                        removed_count += 1\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            if removed_count > 0:\n",
    "                print(f\"✓ Removed {removed_count} existing widgets\")\n",
    "\n",
    "        @staticmethod\n",
    "        def create_base_widgets():\n",
    "            \"\"\"Creates the base widgets needed for file processing\"\"\"\n",
    "            # Text widgets\n",
    "            dbutils.widgets.text(\"container_name\", \"gibsonanalytics/testlake\", \"Blob Storage Container\")\n",
    "            dbutils.widgets.text(\"file_pattern\", \"\", \"File Name\")\n",
    "            dbutils.widgets.text(\"file_encoding\", \"\", \"File Encoding\")\n",
    "            dbutils.widgets.text(\"file_quotechar\", \"\", \"File Quote Character\")\n",
    "            dbutils.widgets.text(\"file_escapechar\", \"\", \"File Escape Character\")\n",
    "            dbutils.widgets.text(\"file_delimiter\", \"\", \"File Delimiter\")\n",
    "            dbutils.widgets.text(\"skip_lines\", \"0\", \"Number of lines to skip\")\n",
    "            dbutils.widgets.text(\"audit_table\", \"file_processing_audit\", \"Audit Table\")\n",
    "            dbutils.widgets.text(\"gc_portco_id\", \"\", \"GC Portfolio Company ID\")\n",
    "            dbutils.widgets.text(\"business_unit_id\", \"\", \"Business Unit ID\")\n",
    "            dbutils.widgets.text(\"sheet_name\", \"\", \"Excel Sheet Name\")\n",
    "            dbutils.widgets.text(\"start_cell\", \"A1\", \"Start Cell (e.g. A1, B4)\")\n",
    "\n",
    "            # Dropdown widget for data domain\n",
    "            dbutils.widgets.dropdown(\n",
    "                \"processing_option\",\n",
    "                \"null\",\n",
    "                [\"null\"] + list(WidgetManager.WIDGET_GROUPS.keys()),\n",
    "                label=\"Select a Data Domain\"\n",
    "            )\n",
    "\n",
    "    def get_config_from_widgets() -> ProcessingConfig:\n",
    "        \"\"\"Gets configuration from widgets\"\"\"\n",
    "        try:\n",
    "            skip_lines = int(dbutils.widgets.get(\"skip_lines\"))\n",
    "        except ValueError:\n",
    "            print(\"Warning: Invalid value for skip_lines. Using 0 as default.\")\n",
    "            skip_lines = 0\n",
    "\n",
    "        return ProcessingConfig(\n",
    "            container_name=dbutils.widgets.get(\"container_name\"),\n",
    "            file_pattern=dbutils.widgets.get(\"file_pattern\"),\n",
    "            encoding=dbutils.widgets.get(\"file_encoding\"),\n",
    "            delimiter=dbutils.widgets.get(\"file_delimiter\"),\n",
    "            quotechar=dbutils.widgets.get(\"file_quotechar\"),\n",
    "            escapechar=dbutils.widgets.get(\"file_escapechar\"),\n",
    "            skip_lines=skip_lines,\n",
    "            audit_table=dbutils.widgets.get(\"audit_table\"),\n",
    "            sheet_name=dbutils.widgets.get(\"sheet_name\"),\n",
    "            excel_starting_cell=dbutils.widgets.get(\"excel_starting_cell\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81919471-8946-4313-8162-b0bd20ffe967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    class FileProcessor:\n",
    "        \"\"\"Processes files in Databricks with built-in monitoring\"\"\"\n",
    "\n",
    "        def __init__(self, spark: SparkSession, config: ProcessingConfig):\n",
    "            self.spark = spark\n",
    "            self.config = config\n",
    "            self.execution_id = str(uuid.uuid4())\n",
    "            self.start_time = datetime.now()\n",
    "            self.domain = None  # Add domain tracking\n",
    "            print(\"\\n\" + \"=\" * 140)\n",
    "            print(f\"Initializing FileProcessor with execution ID: {self.execution_id}\")\n",
    "            print(\"=\" * 140 + \"\\n\")\n",
    "            self._setup_audit_table()\n",
    "\n",
    "        def _setup_audit_table(self):\n",
    "            \"\"\"Creates audit table if it doesn't exist\"\"\"\n",
    "            print(\"Setting up audit table...\")\n",
    "            self.spark.sql(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {self.config.audit_table} (\n",
    "                    execution_id STRING,\n",
    "                    file_name STRING,\n",
    "                    file_size LONG,\n",
    "                    file_modified_timestamp TIMESTAMP,\n",
    "                    start_time TIMESTAMP,\n",
    "                    end_time TIMESTAMP,\n",
    "                    user_name STRING,\n",
    "                    status STRING,\n",
    "                    rows_processed LONG,\n",
    "                    schema_info STRING,\n",
    "                    file_properties STRING,\n",
    "                    processing_details STRING\n",
    "                ) USING DELTA\n",
    "            \"\"\")\n",
    "            print(\"✓ Audit table ready\")\n",
    "\n",
    "        def _sum_numeric_columns(self, df: DataFrame) -> Dict:\n",
    "            \"\"\"\n",
    "            Calculates sum for all numeric columns in one pass.\n",
    "            \"\"\"\n",
    "            numeric_cols = []\n",
    "            for column_name, dtype in df.dtypes:\n",
    "                if any(numeric_type in dtype.lower()\n",
    "                       for numeric_type in ['int', 'bigint', 'double', 'decimal', 'float', 'long']):\n",
    "                    numeric_cols.append(column_name)\n",
    "\n",
    "            if not numeric_cols:\n",
    "                return {}\n",
    "\n",
    "            sums_row = df.agg(*(F.sum(c).alias(c) for c in numeric_cols)).collect()[0]\n",
    "\n",
    "            numeric_sums = {}\n",
    "            for c in numeric_cols:\n",
    "                val = sums_row[c]\n",
    "                if val is not None:\n",
    "                    numeric_sums[c] = float(val)\n",
    "\n",
    "            return numeric_sums\n",
    "\n",
    "        def _analyze_dataframe(self, df: DataFrame) -> Dict:\n",
    "            \"\"\"Analyzes DataFrame schema and content\"\"\"\n",
    "            print(\"\\nAnalyzing DataFrame Schema\")\n",
    "            print(\"-\" * 140)\n",
    "\n",
    "            total_rows = df.count()\n",
    "            numeric_sums = self._sum_numeric_columns(df)\n",
    "\n",
    "            null_counts_row = df.select([\n",
    "                F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns\n",
    "            ]).collect()[0].asDict()\n",
    "\n",
    "            schema_analysis = []\n",
    "            total_columns = len(df.dtypes)\n",
    "\n",
    "            for idx, (column_name, dtype) in enumerate(df.dtypes, 1):\n",
    "                print(f\"Processing column {idx}/{total_columns}: {column_name}\")\n",
    "\n",
    "                null_count = null_counts_row[column_name]\n",
    "                non_null_count = total_rows - null_count\n",
    "                distinct_count = df.select(column_name).distinct().count()\n",
    "\n",
    "                sample_values = (df.select(column_name)\n",
    "                                 .where(col(column_name).isNotNull())\n",
    "                                 .distinct()\n",
    "                                 .orderBy(F.rand())\n",
    "                                 .limit(5)\n",
    "                                 .collect())\n",
    "                sample_values = [str(row[0])[:20] for row in sample_values]\n",
    "\n",
    "                column_info = {\n",
    "                    \"column_name\": column_name,\n",
    "                    \"type\": dtype,\n",
    "                    \"non_null_count\": non_null_count,\n",
    "                    \"null_count\": null_count,\n",
    "                    \"distinct_count\": distinct_count,\n",
    "                    \"sample_values\": sample_values\n",
    "                }\n",
    "\n",
    "                if column_name in numeric_sums:\n",
    "                    column_info[\"sum\"] = numeric_sums[column_name]\n",
    "\n",
    "                schema_analysis.append(column_info)\n",
    "\n",
    "            return {\n",
    "                \"total_rows\": total_rows,\n",
    "                \"total_columns\": len(df.columns),\n",
    "                \"schema_details\": schema_analysis\n",
    "            }\n",
    "\n",
    "        def _display_processing_info(self, file_info: Dict, schema_analysis: Dict, properties: Dict):\n",
    "            \"\"\"Displays processing information in a readable format\"\"\"\n",
    "            print(\"\\n\" + \"=\" * 140)\n",
    "            print(f\"{'File Processing Report - ' + self.execution_id:^140}\")\n",
    "            print(\"=\" * 140 + \"\\n\")\n",
    "\n",
    "            print(\"PROCESSING SUMMARY\")\n",
    "            print(\"-\" * 140)\n",
    "            print(f\"File Name:              {file_info['name']}\")\n",
    "            print(f\"Size:                   {file_info['size']:,} bytes\")\n",
    "            print(f\"Total Rows:             {schema_analysis['total_rows']:,}\")\n",
    "            print(f\"Total Columns:          {schema_analysis['total_columns']}\")\n",
    "            print(f\"Processing Duration:    {str(datetime.now() - self.start_time).split('.')[0]}\")\n",
    "\n",
    "            print(\"\\nFILE PROPERTIES\")\n",
    "            print(\"-\" * 140)\n",
    "            print(f\"Encoding:               {properties['encoding']}\")\n",
    "            print(f\"Delimiter:              {repr(properties['delimiter'])}\")\n",
    "            print(f\"Quote Character:        {repr(properties['quotechar'])}\")\n",
    "            print(\n",
    "                f\"Escape Character:       {repr(properties['escapechar']) if properties['escapechar'] else 'Same as quote character'}\")\n",
    "            print(f\"Skip Lines:             {properties['skip_lines']}\")\n",
    "\n",
    "            print(\"\\nSCHEMA ANALYSIS\")\n",
    "            print(\"-\" * 140)\n",
    "            col_format = \"{:<32} {:<8} {:>12} {:>12} {:>8} {:>20} {:<35}\"\n",
    "            print(col_format.format(\n",
    "                \"Column Name\", \"Type\", \"Non-Null\", \"Distinct\", \"Null %\", \"Sum\", \"Samples\"\n",
    "            ))\n",
    "            print(\"-\" * 140)\n",
    "\n",
    "            for col_info in schema_analysis[\"schema_details\"]:\n",
    "                total = col_info[\"null_count\"] + col_info[\"non_null_count\"]\n",
    "                null_pct = (col_info[\"null_count\"] / total * 100) if total > 0 else 0\n",
    "\n",
    "                formatted_non_null = f\"{col_info['non_null_count']:,}\"\n",
    "                formatted_distinct = f\"{col_info['distinct_count']:,}\"\n",
    "                sum_str = f\"{col_info['sum']:,.2f}\" if 'sum' in col_info and col_info['sum'] is not None else \"\"\n",
    "                samples_str = ', '.join(col_info['sample_values'])\n",
    "\n",
    "                print(col_format.format(\n",
    "                    col_info['column_name'],\n",
    "                    col_info['type'],\n",
    "                    formatted_non_null,\n",
    "                    formatted_distinct,\n",
    "                    f\"{null_pct:>6.1f}%\",\n",
    "                    sum_str,\n",
    "                    samples_str\n",
    "                ))\n",
    "            print(\"=\" * 140 + \"\\n\")\n",
    "\n",
    "        def get_common_encodings(self) -> List[str]:\n",
    "            \"\"\"Returns a list of common encodings that are generally supported\"\"\"\n",
    "            return [\n",
    "                \"Big5\", \"Big5-HKSCS\", \"CESU-8\", \"EUC-JP\", \"EUC-KR\", \"GB18030\", \"GB2312\", \"GBK\", \"IBM-Thai\", \"IBM00858\",\n",
    "                \"IBM01140\", \"IBM01141\", \"IBM01142\", \"IBM01143\", \"IBM01144\", \"IBM01145\", \"IBM01146\", \"IBM01147\",\n",
    "                \"IBM01148\",\n",
    "                \"IBM01149\", \"IBM037\", \"IBM1026\", \"IBM1047\", \"IBM273\", \"IBM277\", \"IBM278\", \"IBM280\", \"IBM284\", \"IBM285\",\n",
    "                \"IBM290\", \"IBM297\", \"IBM420\", \"IBM424\", \"IBM437\", \"IBM500\", \"IBM775\", \"IBM850\", \"IBM852\", \"IBM855\",\n",
    "                \"IBM857\",\n",
    "                \"IBM860\", \"IBM861\", \"IBM862\", \"IBM863\", \"IBM864\", \"IBM865\", \"IBM866\", \"IBM868\", \"IBM869\", \"IBM870\",\n",
    "                \"IBM871\",\n",
    "                \"IBM918\", \"ISO-2022-CN\", \"ISO-2022-JP\", \"ISO-2022-JP-2\", \"ISO-2022-KR\", \"ISO-8859-1\", \"ISO-8859-13\",\n",
    "                \"ISO-8859-15\", \"ISO-8859-2\", \"ISO-8859-3\", \"ISO-8859-4\", \"ISO-8859-5\", \"ISO-8859-6\", \"ISO-8859-7\",\n",
    "                \"ISO-8859-8\",\n",
    "                \"ISO-8859-9\", \"JIS_X0201\", \"JIS_X0212-1990\", \"KOI8-R\", \"KOI8-U\", \"Shift_JIS\", \"TIS-620\", \"US-ASCII\",\n",
    "                \"UTF-16\",\n",
    "                \"UTF-16BE\", \"UTF-16LE\", \"UTF-32\", \"UTF-32BE\", \"UTF-32LE\", \"UTF-8\", \"windows-1250\", \"windows-1251\",\n",
    "                \"windows-1252\", \"windows-1253\", \"windows-1254\", \"windows-1255\", \"windows-1256\", \"windows-1257\",\n",
    "                \"windows-1258\",\n",
    "                \"windows-31j\"\n",
    "            ]\n",
    "\n",
    "        def validate_encoding(self, encoding: str) -> str:\n",
    "            \"\"\"Validates the detected encoding against the list of common encodings\"\"\"\n",
    "            common_encodings = self.get_common_encodings()\n",
    "            if encoding and encoding.upper() in [enc.upper() for enc in common_encodings]:\n",
    "                return encoding\n",
    "            else:\n",
    "                print(f\"Encoding {encoding} not supported. Falling back to UTF-8.\")\n",
    "                return 'UTF-8'\n",
    "\n",
    "        def detect_file_properties(self, file_path: Path) -> Dict:\n",
    "            \"\"\"Detects file encoding and CSV properties\"\"\"\n",
    "            print(\"Detecting file properties...\")\n",
    "\n",
    "            try:\n",
    "                if self.config.encoding:\n",
    "                    validated_encoding = self.validate_encoding(self.config.encoding)\n",
    "                    print(f\"Using user-provided encoding: {validated_encoding}\")\n",
    "                else:\n",
    "                    print(\"Detecting file encoding...\")\n",
    "                    raw_data = file_path.read_bytes()[:3000000]  # Read first 3MB\n",
    "                    result = chardet.detect(raw_data)\n",
    "\n",
    "                    if result and result['confidence'] > 0.5:\n",
    "                        detected_encoding = result['encoding']\n",
    "                        print(f\"Detected encoding: {detected_encoding} with confidence {result['confidence']}\")\n",
    "                        validated_encoding = self.validate_encoding(detected_encoding)\n",
    "                    else:\n",
    "                        print(\"Low confidence in detected encoding. Defaulting to UTF-8.\")\n",
    "                        validated_encoding = 'utf-8'\n",
    "\n",
    "                print(\"Analyzing CSV format...\")\n",
    "                if not (self.config.delimiter and self.config.quotechar):\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding=validated_encoding) as file:\n",
    "                            for _ in range(self.config.skip_lines):\n",
    "                                file.readline()\n",
    "                            content = ''.join([file.readline() for _ in range(5)])\n",
    "\n",
    "                        dialect = csv.Sniffer().sniff(content)\n",
    "                        has_header = csv.Sniffer().has_header(content)\n",
    "\n",
    "                        delimiter = self.config.delimiter or dialect.delimiter\n",
    "                        quotechar = self.config.quotechar or dialect.quotechar\n",
    "                        escapechar = self.config.escapechar\n",
    "\n",
    "                        print(f\"Using Delimiter: {repr(delimiter)}\")\n",
    "                        print(f\"Using Quote Character: {repr(quotechar)}\")\n",
    "                        print(\n",
    "                            f\"Using Escape Character: {repr(escapechar) if escapechar else 'Same as quote character'}\")\n",
    "                        print(f\"Skipping Lines: {self.config.skip_lines}\")\n",
    "                        print(f\"Has Header: {has_header}\")\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error detecting CSV dialect: {str(e)}\")\n",
    "                        delimiter = self.config.delimiter or \",\"\n",
    "                        quotechar = self.config.quotechar or '\"'\n",
    "                        escapechar = self.config.escapechar\n",
    "                else:\n",
    "                    delimiter = self.config.delimiter\n",
    "                    quotechar = self.config.quotechar\n",
    "                    escapechar = self.config.escapechar\n",
    "                    print(f\"Using user-provided delimiter: {repr(delimiter)}\")\n",
    "                    print(f\"Using user-provided quote character: {repr(quotechar)}\")\n",
    "                    print(\n",
    "                        f\"Using user-provided escape character: {repr(escapechar) if escapechar else 'Same as quote character'}\")\n",
    "                    print(f\"Skipping Lines: {self.config.skip_lines}\")\n",
    "\n",
    "                print(\"✓ File properties detected successfully\")\n",
    "                return {\n",
    "                    \"encoding\": validated_encoding,\n",
    "                    \"delimiter\": delimiter,\n",
    "                    \"quotechar\": quotechar,\n",
    "                    \"escapechar\": escapechar,\n",
    "                    \"skip_lines\": self.config.skip_lines\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error detecting file properties: {str(e)}\")\n",
    "                return {\n",
    "                    \"encoding\": \"utf-8\",\n",
    "                    \"delimiter\": \",\",\n",
    "                    \"quotechar\": '\"',\n",
    "                    \"escapechar\": self.config.escapechar,\n",
    "                    \"skip_lines\": self.config.skip_lines\n",
    "                }\n",
    "\n",
    "        def get_latest_file(self) -> Optional[str]:\n",
    "            \"\"\"Gets the most recent file matching the pattern\"\"\"\n",
    "            print(f\"Searching for files matching pattern: {self.config.file_pattern}\")\n",
    "\n",
    "            try:\n",
    "                files = dbutils.fs.ls(f\"/mnt/{self.config.container_name}\")\n",
    "                matching_files = [f for f in files if f.name.startswith(self.config.file_pattern)]\n",
    "\n",
    "                if not matching_files:\n",
    "                    print(f\"No files found matching pattern: {self.config.file_pattern}\")\n",
    "                    return None\n",
    "\n",
    "                latest = sorted(matching_files, key=lambda x: x.modificationTime, reverse=True)[0]\n",
    "                print(f\"✓ Selected file: {latest.name}\")\n",
    "                return latest.name\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error listing files: {str(e)}\")\n",
    "                return None\n",
    "\n",
    "        def process_file(self, file_name: str) -> Optional[DataFrame]:\n",
    "            \"\"\"Main file processing method\"\"\"\n",
    "            self.domain = dbutils.widgets.get(\"processing_option\")\n",
    "            if self.domain != \"null\":\n",
    "                WidgetManager.cleanup_widgets(self.domain)\n",
    "\n",
    "            print(f\"\\nStarting file processing - {file_name}\")\n",
    "\n",
    "            try:\n",
    "                file_path = Path(f\"/dbfs/mnt/{self.config.container_name}/{file_name}\")\n",
    "                file_stats = file_path.stat()\n",
    "                file_modified = datetime.fromtimestamp(file_stats.st_mtime)\n",
    "\n",
    "                file_info = {\n",
    "                    \"name\": file_name,\n",
    "                    \"size\": file_stats.st_size,\n",
    "                    \"modified\": file_modified\n",
    "                }\n",
    "\n",
    "                if file_name.lower().endswith((\".xlsx\", \".xls\")):\n",
    "                    print(\"✓ Detected Excel file format\")\n",
    "\n",
    "                    sheet_name = self.config.sheet_name\n",
    "                    excel_start_cell = self.config.excel_starting_cell\n",
    "\n",
    "                    df = self.spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "                        .option(\"dataAddress\", f\"'{sheet_name}'!{excel_start_cell}\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .option(\"timestampFormat\", \"MM-dd-yyyy HH:mm:ss\") \\\n",
    "                        .load(f\"/mnt/{self.config.container_name}/{file_name}\")\n",
    "\n",
    "                    properties = {\n",
    "                        \"encoding\": \"binary\",\n",
    "                        \"delimiter\": None,\n",
    "                        \"quotechar\": None,\n",
    "                        \"escapechar\": None,\n",
    "                        \"skip_lines\": 0\n",
    "                    }\n",
    "\n",
    "                elif file_name.lower().endswith((\".csv\", \".txt\")):\n",
    "                    print(\"✓ Detected CSV or text file format\")\n",
    "\n",
    "                    properties = self.detect_file_properties(file_path)\n",
    "\n",
    "                    reader = self.spark.read.format(\"csv\") \\\n",
    "                        .option(\"header\", \"true\") \\\n",
    "                        .option(\"inferSchema\", \"true\") \\\n",
    "                        .option(\"delimiter\", properties[\"delimiter\"]) \\\n",
    "                        .option(\"quote\", properties[\"quotechar\"]) \\\n",
    "                        .option(\"multiline\", \"true\") \\\n",
    "                        .option(\"encoding\", properties[\"encoding\"])\n",
    "\n",
    "                    if properties[\"escapechar\"]:\n",
    "                        reader = reader.option(\"escape\", properties[\"escapechar\"])\n",
    "                    else:\n",
    "                        reader = reader.option(\"escape\", properties[\"quotechar\"])\n",
    "\n",
    "                    if properties[\"skip_lines\"] > 0:\n",
    "                        try:\n",
    "                            print(f\"Attempting to skip {properties['skip_lines']} line(s) using 'skipRows' option\")\n",
    "                            df = reader.option(\"skipRows\", properties[\"skip_lines\"]) \\\n",
    "                                .load(f\"/mnt/{self.config.container_name}/{file_name}\")\n",
    "                        except Exception as e1:\n",
    "                            print(f\"Error with skipRows: {str(e1)}\")\n",
    "                            try:\n",
    "                                with open(file_path, 'r', encoding=properties[\"encoding\"]) as f:\n",
    "                                    first_lines = [f.readline().strip() for _ in range(properties[\"skip_lines\"] + 1)]\n",
    "                                comment_char = first_lines[0][0] if first_lines[0] else '#'\n",
    "                                print(f\"Using comment filtering with character: {repr(comment_char)}\")\n",
    "                                with open(file_path, 'r', encoding=properties[\"encoding\"]) as f:\n",
    "                                    content = f.readlines()\n",
    "                                for i in range(min(properties[\"skip_lines\"], len(content))):\n",
    "                                    if not content[i].startswith(comment_char):\n",
    "                                        content[i] = comment_char + content[i]\n",
    "                                temp_file_path = file_path.with_suffix('.temp.csv')\n",
    "                                with open(temp_file_path, 'w', encoding=properties[\"encoding\"]) as f:\n",
    "                                    f.writelines(content)\n",
    "                                df = reader.option(\"comment\", comment_char) \\\n",
    "                                    .load(f\"/mnt/{self.config.container_name}/{file_name}.temp\")\n",
    "                                temp_file_path.unlink(missing_ok=True)\n",
    "                            except Exception as e2:\n",
    "                                print(f\"Error with comment workaround: {str(e2)}\")\n",
    "                                print(\"Falling back to loading all data and dropping first rows\")\n",
    "                                df = reader.load(f\"/mnt/{self.config.container_name}/{file_name}\")\n",
    "                                df = df.limit(df.count() - properties[\"skip_lines\"])\n",
    "                    else:\n",
    "                        df = reader.load(f\"/mnt/{self.config.container_name}/{file_name}\")\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file type: {file_name}\")\n",
    "\n",
    "                analysis = self._analyze_dataframe(df)\n",
    "                self._display_processing_info(file_info, analysis, properties)\n",
    "\n",
    "                user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "                self.spark.createDataFrame([(\n",
    "                    self.execution_id,\n",
    "                    file_name,\n",
    "                    file_stats.st_size,\n",
    "                    file_modified,\n",
    "                    self.start_time,\n",
    "                    datetime.now(),\n",
    "                    user,\n",
    "                    \"SUCCESS\",\n",
    "                    analysis[\"total_rows\"],\n",
    "                    json.dumps(analysis[\"schema_details\"]),\n",
    "                    json.dumps(properties),\n",
    "                    json.dumps({\"file_processing_successful\": True})\n",
    "                )]).toDF(\n",
    "                    \"execution_id\", \"file_name\", \"file_size\", \"file_modified_timestamp\",\n",
    "                    \"start_time\", \"end_time\", \"user_name\", \"status\", \"rows_processed\",\n",
    "                    \"schema_info\", \"file_properties\", \"processing_details\"\n",
    "                ).write.mode(\"append\").saveAsTable(self.config.audit_table)\n",
    "\n",
    "                print(\"✓ File processing completed successfully\")\n",
    "                return df\n",
    "\n",
    "            except Exception as e:\n",
    "                error_msg = str(e)\n",
    "                print(\"\\nError processing file\")\n",
    "                print(f\"File: {file_name}\")\n",
    "                print(f\"Error: {error_msg}\")\n",
    "\n",
    "                user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "                self.spark.createDataFrame([(\n",
    "                    self.execution_id,\n",
    "                    file_name,\n",
    "                    file_stats.st_size if 'file_stats' in locals() else None,\n",
    "                    file_modified if 'file_modified' in locals() else None,\n",
    "                    self.start_time,\n",
    "                    datetime.now(),\n",
    "                    user,\n",
    "                    \"ERROR\",\n",
    "                    0,\n",
    "                    None,\n",
    "                    json.dumps(properties) if 'properties' in locals() else None,\n",
    "                    json.dumps({\"error\": error_msg})\n",
    "                )]).toDF(\n",
    "                    \"execution_id\", \"file_name\", \"file_size\", \"file_modified_timestamp\",\n",
    "                    \"start_time\", \"end_time\", \"user_name\", \"status\", \"rows_processed\",\n",
    "                    \"schema_info\", \"file_properties\", \"processing_details\"\n",
    "                ).write.mode(\"append\").saveAsTable(self.config.audit_table)\n",
    "\n",
    "                return None\n",
    "\n",
    "    def create_file_processor(spark: SparkSession) -> FileProcessor:\n",
    "        \"\"\"Creates FileProcessor instance from widgets\"\"\"\n",
    "\n",
    "        # Always create base widgets first\n",
    "        WidgetManager.create_base_widgets()\n",
    "\n",
    "        # Now that widgets are guaranteed to exist, get config\n",
    "        config = get_config_from_widgets()\n",
    "\n",
    "        # Initialize FileProcessor\n",
    "        processor = FileProcessor(spark, config)\n",
    "\n",
    "        return processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3409c83-15b5-4f9a-a11b-9279a02c3b74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    if __name__ == \"__main__\":\n",
    "        # Create processor instance\n",
    "        processor = create_file_processor(spark)\n",
    "\n",
    "        # Process latest file\n",
    "        if latest_file := processor.get_latest_file():\n",
    "            if df := processor.process_file(latest_file):\n",
    "                print(\"\\nPreview of processed data:\")\n",
    "                display(df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49185b5c-3772-4f9d-91d8-a903b1399a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to sanitize column names\n",
    "def sanitize_column_name(name):\n",
    "    return re.sub(r'[^0-9a-zA-Z$]+', '_', name).lower()\n",
    "\n",
    "# Getting unique column names after sanitization\n",
    "unique_names = set()\n",
    "new_column_names = {}\n",
    "\n",
    "for original_name in df.columns:\n",
    "    sanitized_name = sanitize_column_name(original_name)\n",
    "    unique_name = sanitized_name\n",
    "    count = 1\n",
    "\n",
    "    # Ensure the uniqueness of the column name\n",
    "    while unique_name in unique_names:\n",
    "        unique_name = f\"{sanitized_name}_{count}\"\n",
    "        count += 1\n",
    "\n",
    "    unique_names.add(unique_name)\n",
    "    new_column_names[original_name] = unique_name\n",
    "\n",
    "# Rename columns\n",
    "for original_name, new_name in new_column_names.items():\n",
    "    df = df.withColumnRenamed(original_name, new_name)\n",
    "\n",
    "# Optionally, print the updated column names to verify\n",
    "print(\"Updated column names:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65233c5a-bbce-4854-9ede-4b820e6e4b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Count the number of rows before removing duplicates\n",
    "    row_count_before = df.count()\n",
    "\n",
    "    # Drop duplicate rows (keeps the first occurrence of duplicate rows)\n",
    "    df_deduplicated = df.dropDuplicates()\n",
    "\n",
    "    # Count the number of rows after removing duplicates\n",
    "    row_count_after = df_deduplicated.count()\n",
    "\n",
    "    # Calculate the number of duplicate rows\n",
    "    total_duplicate_rows = row_count_before - row_count_after\n",
    "\n",
    "    if total_duplicate_rows > 0:\n",
    "        print(f\"Total number of duplicate rows removed: {total_duplicate_rows}\")\n",
    "        df = df_deduplicated\n",
    "    else:\n",
    "        print(\"No duplicate rows found. No action required.\")\n",
    "\n",
    "    # Show the count of rows after potential deduplication\n",
    "    print(f\"Number of rows after deduplication: {row_count_after}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during duplicate row removal: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "247131ff-7d9b-4608-bd3c-7424c7dce286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get widget values directly\n",
    "    gc_portco_id = dbutils.widgets.get(\"gc_portco_id\")\n",
    "    business_unit_id = dbutils.widgets.get(\"business_unit_id\")\n",
    "    ingested_by = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "    # Add the SRC_FILE column with the name of the source file\n",
    "    df = df.withColumn(\"SRC_FILE\", lit(latest_file))\n",
    "    print(\"Added SRC_FILE column to DataFrame.\")\n",
    "\n",
    "    # Add the EXECUTION_ID from the FileProcessor\n",
    "    df = df.withColumn(\"EXECUTION_ID\", lit(processor.execution_id))\n",
    "    print(\"Added EXECUTION_ID column to DataFrame with value:\", processor.execution_id)\n",
    "\n",
    "    # Add the GC_PORTCO_ID column using the value from the widget\n",
    "    df = df.withColumn(\"GC_PORTCO_ID\", lit(gc_portco_id))\n",
    "    print(\"Added GC_PORTCO_ID column to DataFrame with value:\", gc_portco_id)\n",
    "\n",
    "    # Add the BUSINESS_UNIT_ID column using the value from the widget\n",
    "    df = df.withColumn(\"BUSINESS_UNIT_ID\", lit(business_unit_id))\n",
    "    print(\"Added BUSINESS_UNIT_ID column to DataFrame with value:\", business_unit_id)\n",
    "\n",
    "    # Add the INGESTED_BY column using the current user\n",
    "    df = df.withColumn(\"INGESTED_BY\", lit(ingested_by))\n",
    "    print(\"Added INGESTED_BY column to DataFrame with value:\", ingested_by)\n",
    "\n",
    "    # Add the INGESTION_DT column with the current timestamp\n",
    "    df = df.withColumn(\"INGESTION_DT\", current_timestamp())\n",
    "    print(\"Added INGESTION_DT column to DataFrame.\")\n",
    "\n",
    "    # Display a sample of 20 records using display()\n",
    "    print(\"\\nDisplaying a 20 record sample of the DataFrame:\")\n",
    "    display(df.limit(20))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error modifying DataFrame: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f017527e-3b1f-489e-8e92-72292acd8685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Starting to trim leading and trailing spaces from each column.\")\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for col_name in df.columns:\n",
    "        # 'trim' function is applied to each column to remove leading and trailing spaces\n",
    "        df = df.withColumn(col_name, trim(df[col_name]))\n",
    "\n",
    "    print(\"Completed trimming spaces from all columns.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during trimming process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1260201f-b281-4710-a558-519dbdf1d22d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class WidgetManager:\n",
    "    \"\"\"Manages the lifecycle of Databricks widgets\"\"\"\n",
    "\n",
    "    # Widget definitions by domain\n",
    "    WIDGET_GROUPS = {\n",
    "        \"Accounts Payable\": [\n",
    "            \"INVOICE_NUMBER\",\n",
    "            \"INVOICE_LINE_NUM\",\n",
    "            \"INVOICE_DATE\",\n",
    "            \"SUPPLIER_MASTER_ID\",\n",
    "            \"SUPPLIER_NAME\",\n",
    "            \"INVOICE_NET_AMOUNT\",\n",
    "            \"INVOICE_NET_AMOUNT_USD\",\n",
    "            \"INVOICE_QTY\",\n",
    "            \"RECEIVED_DATE\",\n",
    "            \"confirm_mapping\"\n",
    "        ],\n",
    "        \"Purchase Orders\": [\n",
    "            \"PURCHASE_ORDER_NUM\",\n",
    "            \"PURCHASE_ORDER_LINE_NUM\",\n",
    "            \"PURCHASE_ORDER_DATE\",\n",
    "            \"SUPPLIER_MASTER_ID\",\n",
    "            \"SUPPLIER_NAME\",\n",
    "            \"TOTAL_SPEND_LOCAL\",\n",
    "            \"TOTAL_SPEND_USD\",\n",
    "            \"RECEIVED_DATE\",\n",
    "            \"confirm_mapping\"\n",
    "        ],\n",
    "        \"Item Master\": [\n",
    "            \"PART_NUMBER\",\n",
    "            \"PART_NAME\",\n",
    "            \"PART_DESCRIPTION\",\n",
    "            \"confirm_mapping\"\n",
    "        ],\n",
    "        \"Supplier Master\": [\n",
    "            \"SUPPLIER_ID\",\n",
    "            \"SUPPLIER_NAME\",\n",
    "            \"confirm_mapping\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    GENERAL_REPORT_WIDGETS = [\n",
    "        \"MAPPED_SUPPLIER_NAME\",\n",
    "        \"MAPPED_PART_NUMBER\",\n",
    "        \"MAPPED_TOTAL_SPEND\",\n",
    "        \"confirm_general_mapping\"\n",
    "    ]\n",
    "\n",
    "    @staticmethod\n",
    "    def check_widget_exists(widget_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Safely check if a widget exists\n",
    "\n",
    "        Args:\n",
    "            widget_name: Name of widget to check\n",
    "\n",
    "        Returns:\n",
    "            bool: True if widget exists, False otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dbutils.widgets.get(widget_name)\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_widgets(domain: str = None):\n",
    "        \"\"\"\n",
    "        Cleans up widgets based on domain. If no domain specified, cleans up all known widgets.\n",
    "\n",
    "        Args:\n",
    "            domain (str, optional): Specific domain whose widgets should be cleaned up\n",
    "        \"\"\"\n",
    "        if domain and domain in WidgetManager.WIDGET_GROUPS:\n",
    "            # Clean up specific domain widgets\n",
    "            widgets_to_remove = WidgetManager.WIDGET_GROUPS[domain]\n",
    "            print(f\"Cleaning up widgets for domain: {domain}\")\n",
    "        else:\n",
    "            # Clean up all known widgets\n",
    "            widgets_to_remove = []\n",
    "            for group in WidgetManager.WIDGET_GROUPS.values():\n",
    "                widgets_to_remove.extend(group)\n",
    "            print(\"Cleaning up all mapping widgets...\")\n",
    "\n",
    "        removed_count = 0\n",
    "        for widget in widgets_to_remove:\n",
    "            if WidgetManager.check_widget_exists(widget):\n",
    "                try:\n",
    "                    dbutils.widgets.remove(widget)\n",
    "                    removed_count += 1\n",
    "                except Exception:\n",
    "                    pass  # Ignore errors during removal\n",
    "\n",
    "        if removed_count > 0:\n",
    "            print(f\"✓ Removed {removed_count} existing widgets\")\n",
    "\n",
    "    @staticmethod\n",
    "    def cleanup_general_widgets():\n",
    "        for widget in WidgetManager.GENERAL_REPORT_WIDGETS:\n",
    "            if WidgetManager.check_widget_exists(widget):\n",
    "                dbutils.widgets.remove(widget)\n",
    "\n",
    "\n",
    "class WidgetMappingManager:\n",
    "    \"\"\"Manages the creation and mapping of domain-specific widgets\"\"\"\n",
    "\n",
    "    # Define the required fields for each domain\n",
    "    DOMAIN_MAPPINGS = {\n",
    "        \"Purchase Orders\": {\n",
    "            \"PURCHASE_ORDER_NUM\": \"Map: Purchase Order Number\",\n",
    "            \"PURCHASE_ORDER_LINE_NUM\": \"Map: Purchase Order Line Number\",\n",
    "            \"PURCHASE_ORDER_DATE\": \"Map: Purchase Order Date\",\n",
    "            \"SUPPLIER_MASTER_ID\": \"Map: Supplier Master ID\",\n",
    "            \"SUPPLIER_NAME\": \"Map: Supplier Name\",\n",
    "            \"TOTAL_SPEND_LOCAL\": \"Map: Total Spend Local\",\n",
    "            \"TOTAL_SPEND_USD\": \"Map: Total Spend USD\",\n",
    "            \"RECEIVED_DATE\": \"Map: Received Date\"\n",
    "        },\n",
    "        \"Accounts Payable\": {\n",
    "            \"INVOICE_NUMBER\": \"Map: Invoice Number\",\n",
    "            \"INVOICE_LINE_NUM\": \"Map: Invoice Date\",\n",
    "            \"INVOICE_DATE\": \"Map: Due Date\",\n",
    "            \"SUPPLIER_MASTER_ID\": \"Map: Supplier Master ID\",\n",
    "            \"SUPPLIER_NAME\": \"Map: Supplier Name\",\n",
    "            \"INVOICE_NET_AMOUNT\": \"Map: Invoice Amount\",\n",
    "            \"INVOICE_NET_AMOUNT_USD\": \"Map: Invoice Amount USD\",\n",
    "            \"RECEIVED_DATE\": \"Map: Payment Status\"\n",
    "        },\n",
    "        \"Item Master\": {\n",
    "            \"PART_NUMBER\": \"Map: Item Number\",\n",
    "            \"PART_NAME\": \"Map: Item Description\",\n",
    "            \"PART_DESCRIPTION\": \"Map: Item Category\"\n",
    "        },\n",
    "        \"Supplier Master\": {\n",
    "            \"SUPPLIER_ID\": \"Map: Supplier ID\",\n",
    "            \"SUPPLIER_NAME\": \"Map: Supplier Name\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mapping_widgets(df, domain: str):\n",
    "        \"\"\"\n",
    "        Creates mapping widgets for the specified domain using DataFrame columns\n",
    "\n",
    "        Args:\n",
    "            df: Spark DataFrame containing the source data\n",
    "            domain: The data domain to create widgets for\n",
    "        \"\"\"\n",
    "        print(f\"\\nCreating mapping widgets for domain: {domain}\")\n",
    "\n",
    "        if domain not in WidgetMappingManager.DOMAIN_MAPPINGS:\n",
    "            print(f\"Error: Unknown domain '{domain}'. No mapping widgets created.\")\n",
    "            return\n",
    "\n",
    "        # Get column list from DataFrame and add empty option\n",
    "        column_list = [\"\"] + df.columns\n",
    "        print(f\"Available columns: {column_list}\")\n",
    "\n",
    "        # Create widgets for the specified domain\n",
    "        domain_fields = WidgetMappingManager.DOMAIN_MAPPINGS[domain]\n",
    "        for field_name, field_label in domain_fields.items():\n",
    "            dbutils.widgets.dropdown(field_name, \"\", column_list, field_label)\n",
    "\n",
    "        # Always add the confirmation widget\n",
    "        dbutils.widgets.dropdown(\"confirm_mapping\", \"no\", [\"no\", \"yes\"], \"Set to 'yes' to apply mapping\")\n",
    "\n",
    "        print(f\"✓ Created {len(domain_fields)} mapping widgets for {domain}\")\n",
    "        print(\"Please make your selections at the top of the notebook.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_field_mappings(domain: str) -> dict:\n",
    "        \"\"\"\n",
    "        Gets the current field mappings for the specified domain\n",
    "\n",
    "        Args:\n",
    "            domain: The data domain to get mappings for\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary of field mappings {target_field: source_column}\n",
    "        \"\"\"\n",
    "        if domain not in WidgetMappingManager.DOMAIN_MAPPINGS:\n",
    "            return {}\n",
    "\n",
    "        mappings = {}\n",
    "        for field_name in WidgetMappingManager.DOMAIN_MAPPINGS[domain].keys():\n",
    "            mapped_column = dbutils.widgets.get(field_name)\n",
    "            if mapped_column:  # Only include non-empty mappings\n",
    "                mappings[field_name] = mapped_column\n",
    "\n",
    "        return mappings\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def create_mapping_widgets_for_domain(df, domain: str = None):\n",
    "    \"\"\"\n",
    "    Creates mapping widgets based on the selected domain\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame containing the source data\n",
    "        domain: Optional domain override, otherwise uses widget value\n",
    "    \"\"\"\n",
    "    # Get domain from widget if not specified\n",
    "    if domain is None:\n",
    "        domain = dbutils.widgets.get(\"processing_option\")\n",
    "\n",
    "    if domain == \"null\":\n",
    "        print(\"No domain selected. Please select a data domain first.\")\n",
    "        return\n",
    "\n",
    "    # Clean up ALL existing mapping widgets, not just the current domain\n",
    "    WidgetManager.cleanup_widgets()  # No domain parameter means clean up all\n",
    "\n",
    "    # Create new mapping widgets\n",
    "    WidgetMappingManager.create_mapping_widgets(df, domain)\n",
    "\n",
    "\n",
    "def create_general_report_widgets(df):\n",
    "    columns = [\"\"] + df.columns\n",
    "    WidgetManager.cleanup_general_widgets()\n",
    "\n",
    "    # Get the current domain and any mapped widget values\n",
    "    try:\n",
    "        domain = dbutils.widgets.get(\"processing_option\")\n",
    "        domain_mappings = WidgetMappingManager.get_field_mappings(domain)\n",
    "    except:\n",
    "        domain = None\n",
    "        domain_mappings = {}\n",
    "\n",
    "    # Helper to get the actual selected value from a domain widget if it exists\n",
    "    def get_widget_value_safe(widget_name):\n",
    "        try:\n",
    "            return dbutils.widgets.get(widget_name)\n",
    "        except:\n",
    "            return \"\"\n",
    "\n",
    "    # Only create general widgets if their equivalent domain values are blank\n",
    "    if not get_widget_value_safe(\"SUPPLIER_NAME\"):\n",
    "        dbutils.widgets.dropdown(\"MAPPED_SUPPLIER_NAME\", \"\", columns, \"Map: Supplier Name\")\n",
    "\n",
    "    if not get_widget_value_safe(\"PART_NUMBER\"):\n",
    "        dbutils.widgets.dropdown(\"MAPPED_PART_NUMBER\", \"\", columns, \"Map: SKU / Part Number\")\n",
    "\n",
    "    # Spend column logic: check all domain-level options\n",
    "    spend_mapped = any([\n",
    "        get_widget_value_safe(\"INVOICE_NET_AMOUNT\"),\n",
    "        get_widget_value_safe(\"TOTAL_SPEND_USD\"),\n",
    "        get_widget_value_safe(\"TOTAL_SPEND_LOCAL\")\n",
    "    ])\n",
    "\n",
    "    if not spend_mapped:\n",
    "        dbutils.widgets.dropdown(\"MAPPED_TOTAL_SPEND\", \"\", columns, \"Map: Spend Column\")\n",
    "\n",
    "    # Always create confirm_mapping if it doesn't exist\n",
    "    if not WidgetManager.check_widget_exists(\"confirm_mapping\"):\n",
    "        dbutils.widgets.dropdown(\"confirm_mapping\", \"no\", [\"no\", \"yes\"], \"Set to 'yes' to apply mappings\")\n",
    "\n",
    "    print(\"✓ General ingestion mapping widgets created (if needed).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a5e41ace-1864-4d06-a140-ba60848a5082",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create mapping widgets based on the selected domain\n",
    "create_mapping_widgets_for_domain(df)  # domain-specific mappings\n",
    "create_general_report_widgets(df)      # general-purpose mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aea4dee-ace0-4cfd-bfd8-2d9f70986a17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_widget_mappings(df, domain: str = None):\n",
    "    \"\"\"\n",
    "    Applies the widget mappings to create a new DataFrame with standardized column names\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame containing the source data\n",
    "        domain: Optional domain override, otherwise uses widget value\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: New DataFrame with mapped column names, or None if mappings not confirmed\n",
    "    \"\"\"\n",
    "    print(\"=\" * 132)\n",
    "    print(\"Applying Widget Mappings\".center(132))\n",
    "    print(\"=\" * 132)\n",
    "    print()\n",
    "\n",
    "    # Get domain from widget if not specified\n",
    "    if domain is None:\n",
    "        domain = dbutils.widgets.get(\"processing_option\")\n",
    "\n",
    "    if domain == \"null\":\n",
    "        print(\"Error: No domain selected. Please select a data domain first.\")\n",
    "        return None\n",
    "\n",
    "    # Check if mappings are confirmed\n",
    "    if dbutils.widgets.get(\"confirm_mapping\") != \"yes\":\n",
    "        print(\"Warning: Mappings not confirmed. Please set confirm_mapping to 'yes' to apply mappings.\")\n",
    "        return None\n",
    "\n",
    "    # Get current mappings\n",
    "    mappings = WidgetMappingManager.get_field_mappings(domain)\n",
    "\n",
    "    if not mappings:\n",
    "        print(f\"Error: No mappings found for domain '{domain}'\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Processing mappings for domain: {domain}\")\n",
    "    print(\"-\" * 132)\n",
    "\n",
    "    print(\"\\nField Mappings\")\n",
    "    print(\"-\" * 132)\n",
    "    col_width = max(len(max(mappings.keys(), key=len)), len(max(mappings.values(), key=len))) + 2\n",
    "    print(f\"{'Source Column'.ljust(col_width)} → {'Target Field'.ljust(col_width)}\")\n",
    "    print(\"-\" * (col_width * 2 + 3))\n",
    "    for target_field, source_column in mappings.items():\n",
    "        print(f\"{source_column.ljust(col_width)} → {target_field.ljust(col_width)}\")\n",
    "\n",
    "    # Create new DataFrame with mapped columns\n",
    "    mapped_df = df.select([\n",
    "        df[source_column].alias(target_field)\n",
    "        for target_field, source_column in mappings.items()\n",
    "    ])\n",
    "\n",
    "    print(\"\\nOutput Schema\")\n",
    "    print(\"-\" * 132)\n",
    "    mapped_df.printSchema()\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 132)\n",
    "    print(f\"✓ Successfully created DataFrame with {len(mappings)} mapped columns\".center(132))\n",
    "    print(\"=\" * 132)\n",
    "\n",
    "    return mapped_df\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "mapped_df = apply_widget_mappings(df)  # domain logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9ef791d5-121f-4fec-a609-687b3f8a7b0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_general_ingestion_report(df):\n",
    "    from pyspark.sql.functions import col, regexp_replace, when\n",
    "\n",
    "    if dbutils.widgets.get(\"confirm_mapping\") != \"yes\":\n",
    "        print(\"⚠ Please confirm mapping first by setting 'confirm_mapping' to yes.\")\n",
    "        return\n",
    "\n",
    "    # Try general mapping widgets first\n",
    "    use_general = WidgetManager.check_widget_exists(\"MAPPED_TOTAL_SPEND\")\n",
    "\n",
    "    if use_general:\n",
    "        supplier_col = dbutils.widgets.get(\"MAPPED_SUPPLIER_NAME\")\n",
    "        part_col = dbutils.widgets.get(\"MAPPED_PART_NUMBER\")\n",
    "        spend_col = dbutils.widgets.get(\"MAPPED_TOTAL_SPEND\")\n",
    "    else:\n",
    "        # Use domain mappings\n",
    "        domain = dbutils.widgets.get(\"processing_option\")\n",
    "        domain_mappings = WidgetMappingManager.get_field_mappings(domain)\n",
    "\n",
    "        supplier_col = domain_mappings.get(\"SUPPLIER_NAME\")\n",
    "        part_col = (\n",
    "                domain_mappings.get(\"PART_NUMBER\") or\n",
    "                dbutils.widgets.get(\"MAPPED_PART_NUMBER\")  # fallback to general if PART_NUMBER isn't mapped\n",
    "        )\n",
    "        spend_col = (\n",
    "                domain_mappings.get(\"INVOICE_NET_AMOUNT\") or\n",
    "                domain_mappings.get(\"TOTAL_SPEND_USD\") or\n",
    "                domain_mappings.get(\"TOTAL_SPEND_LOCAL\") or\n",
    "                dbutils.widgets.get(\"MAPPED_TOTAL_SPEND\")  # fallback\n",
    "        )\n",
    "\n",
    "    if not (supplier_col and part_col and spend_col):\n",
    "        print(f\"⚠ Missing mappings → supplier_col: {supplier_col}, part_col: {part_col}, spend_col: {spend_col}\")\n",
    "        print(\"⚠ Missing mappings for supplier, part, or spend column.\")\n",
    "        return\n",
    "\n",
    "    print(\"=\" * 132)\n",
    "    print(\"General Ingestion Report\".center(132))\n",
    "    print(\"=\" * 132)\n",
    "\n",
    "    dedup_df = df.dropDuplicates()\n",
    "    total_rows = df.count()\n",
    "    dedup_rows = dedup_df.count()\n",
    "    duplicate_count = total_rows - dedup_rows\n",
    "\n",
    "    from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "    casted_df = dedup_df.withColumn(\n",
    "        spend_col,\n",
    "        when(col(spend_col).rlike(r\"\\(.*\\)\"),\n",
    "             # Convert (1234.56) to -1234.56\n",
    "             regexp_replace(regexp_replace(col(spend_col), \"[()]\", \"\"), \"[$,]\", \"\").cast(\"double\") * -1\n",
    "             ).otherwise(\n",
    "            # Normal positive number\n",
    "            regexp_replace(col(spend_col), \"[$,]\", \"\").cast(\"double\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    total_spend = casted_df.select(spend_col).na.drop().groupBy().sum(spend_col).collect()[0][0]\n",
    "    unique_suppliers = dedup_df.select(supplier_col).distinct().count()\n",
    "    unique_skus = dedup_df.select(part_col).distinct().count()\n",
    "\n",
    "    print(f\"Total Rows Read: {total_rows}\")\n",
    "    print(f\"Duplicate Rows Removed: {duplicate_count}\")\n",
    "    print(f\"Total Spend: {total_spend}\")\n",
    "    print(f\"Unique Suppliers: {unique_suppliers}\")\n",
    "    print(f\"Unique SKUs: {unique_skus}\")\n",
    "    print(\"=\" * 132)\n",
    "    # Try to get file name\n",
    "    try:\n",
    "        src_file = dbutils.widgets.get(\"file_pattern\")\n",
    "    except:\n",
    "        src_file = \"unknown\"\n",
    "\n",
    "    # Create summary_df using df.select(...) to match mapped_df style\n",
    "    summary_df = df.limit(1).select(\n",
    "        lit(src_file).alias(\"SRC_FILE\"),\n",
    "        lit(total_rows).cast(\"BIGINT\").alias(\"TOTAL_ROWS\"),\n",
    "        lit(duplicate_count).cast(\"BIGINT\").alias(\"DUPLICATE_ROWS_REMOVED\"),\n",
    "        lit(total_spend).cast(\"DOUBLE\").alias(\"TOTAL_SPEND\"),\n",
    "        lit(unique_suppliers).cast(\"BIGINT\").alias(\"UNIQUE_SUPPLIERS\"),\n",
    "        lit(unique_skus).cast(\"BIGINT\").alias(\"UNIQUE_SKUS\"),\n",
    "        current_timestamp().alias(\"INGESTION_TIMESTAMP\")\n",
    "    )\n",
    "\n",
    "    # ✅ Append to Delta table\n",
    "    summary_df.write.mode(\"append\").format(\"delta\").saveAsTable(\"gibsonanalytics.ingestion.audit_ingestion_summary\")\n",
    "\n",
    "    # Show in notebook\n",
    "    print(\"\\n📄 Summary of Ingestion Metrics:\")\n",
    "    display(summary_df)\n",
    "\n",
    "\n",
    "generate_general_ingestion_report(df)  # general insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5492deb2-d29a-4796-b8b4-cb54e7929591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"api_mapping_table\", \"api mapping table\", \"API Mapping Table\")\n",
    "api_mapping_table = dbutils.widgets.get(\"api_mapping_table\")\n",
    "\n",
    "try:\n",
    "    # Write the DataFrame to a Delta table\n",
    "    mapped_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(api_mapping_table)\n",
    "\n",
    "    print(f\"✓ Successfully wrote data to Delta table\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to Delta table: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff49dcf-f0ae-4539-bada-6b1f592de342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve widget values. (These widgets can be defined in an earlier cell.)\n",
    "dbutils.widgets.text(\"max_new_column_percentage\", \"0.4\", \"Max New Column Percentage\")\n",
    "dbutils.widgets.text(\"file_pattern\", \"myfile\", \"File Pattern\")\n",
    "dbutils.widgets.text(\"api_mapping_table\", \"gibsonanalytics.ingestion.api_mapping\", \"API Mapping Table\")\n",
    "\n",
    "max_new_column_percentage     = float(dbutils.widgets.get(\"max_new_column_percentage\"))\n",
    "business_unit_id              = dbutils.widgets.get(\"business_unit_id\")\n",
    "file_pattern                  = dbutils.widgets.get(\"file_pattern\")\n",
    "api_mapping_table             = dbutils.widgets.get(\"api_mapping_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99aff06d-d1e2-4388-aee9-7078587d0f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType\n",
    "from typing import Tuple, Dict, Any\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Retrieve table identifiers from widgets\n",
    "dbutils.widgets.text(\"catalog_name\", \"\", \"Bronze Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"\", \"Bronze Schema Name\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Bronze Table Name\")\n",
    "\n",
    "\n",
    "catalog_name = dbutils.widgets.get(\"catalog_name\")\n",
    "schema_name = dbutils.widgets.get(\"schema_name\")\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{table_name}\"\n",
    "\n",
    "# Configure a dedicated logger\n",
    "logger = logging.getLogger(\"DeltaTableLogger\")\n",
    "if not logger.handlers:\n",
    "    logger.setLevel(logging.INFO)\n",
    "    handler = logging.StreamHandler(sys.stdout)\n",
    "    formatter = logging.Formatter(fmt=\"%(asctime)s - %(levelname)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "\n",
    "def get_schema_metrics(existing_schema: StructType, new_schema: StructType,\n",
    "                      max_new_column_percentage: float,\n",
    "                      min_schema_match_percentage: float,\n",
    "                      validation_result: Tuple[bool, str]) -> Dict[str, Any]:\n",
    "    existing_fields = {field.name for field in existing_schema.fields}\n",
    "    new_fields = {field.name for field in new_schema.fields}\n",
    "    new_columns = new_fields - existing_fields\n",
    "    matching_columns = existing_fields.intersection(new_fields)\n",
    "    existing_count = len(existing_fields)\n",
    "    new_column_count = len(new_columns)\n",
    "    matching_count = len(matching_columns)\n",
    "    new_column_percentage = new_column_count / existing_count if existing_count > 0 else 0\n",
    "    match_percentage = matching_count / existing_count if existing_count > 0 else 1.0\n",
    "\n",
    "    is_valid, msg = validation_result\n",
    "    if not is_valid:\n",
    "        if \"too many new columns\" in msg.lower():\n",
    "            validation_status = \"ERROR: Too many new columns\"\n",
    "        elif \"doesn't match enough existing columns\" in msg.lower():\n",
    "            validation_status = \"ERROR: Insufficient column match\"\n",
    "        else:\n",
    "            validation_status = \"ERROR: Schema validation failed\"\n",
    "    else:\n",
    "        validation_status = \"SUCCESS: Schema validation passed\"\n",
    "\n",
    "    return {\n",
    "        \"validation_status\": validation_status,\n",
    "        \"new_columns\": list(sorted(new_columns)),\n",
    "        \"new_column_count\": new_column_count,\n",
    "        \"new_column_percentage\": f\"{new_column_percentage:.2%}\",\n",
    "        \"max_allowed_new_column_percentage\": f\"{max_new_column_percentage:.2%}\",\n",
    "        \"matching_columns_count\": matching_count,\n",
    "        \"match_percentage\": f\"{match_percentage:.2%}\",\n",
    "        \"min_required_match_percentage\": f\"{min_schema_match_percentage:.2%}\",\n",
    "        \"total_columns\": len(new_fields),\n",
    "        \"schema_validation_timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def validate_schema_changes(existing_schema: StructType, new_schema: StructType,\n",
    "                          max_new_column_percentage: float = 0.4,\n",
    "                          min_schema_match_percentage: float = 0.8) -> Tuple[bool, str]:\n",
    "    existing_fields = {field.name for field in existing_schema.fields}\n",
    "    new_fields = {field.name for field in new_schema.fields}\n",
    "\n",
    "    if len(existing_fields) == 0:\n",
    "        msg = \"No existing schema to compare; skipping validation.\"\n",
    "        logger.info(msg)\n",
    "        return True, msg\n",
    "\n",
    "    matching_columns = existing_fields.intersection(new_fields)\n",
    "    match_percentage = len(matching_columns) / len(existing_fields)\n",
    "\n",
    "    if match_percentage < min_schema_match_percentage:\n",
    "        error_msg = (\n",
    "            \"\\n\" + \"=\"*80 + \"\\n\" +\n",
    "            \"SCHEMA VALIDATION ERROR\\n\" +\n",
    "            \"=\"*80 + \"\\n\" +\n",
    "            \"The incoming DataFrame schema doesn't match enough existing columns.\\n\\n\" +\n",
    "            f\"Existing table column count: {len(existing_fields)}\\n\" +\n",
    "            f\"Matching columns: {len(matching_columns)}\\n\" +\n",
    "            f\"Match percentage: {match_percentage:.2%}\\n\" +\n",
    "            f\"Minimum required: {min_schema_match_percentage:.2%}\\n\\n\" +\n",
    "            f\"Missing columns that would be populated with nulls:\\n\" +\n",
    "            \"\\n\".join(f\"  - {col}\" for col in sorted(existing_fields - matching_columns)) + \"\\n\" +\n",
    "            \"=\"*80\n",
    "        )\n",
    "        logger.error(error_msg)\n",
    "        return False, error_msg\n",
    "\n",
    "    new_columns = new_fields - existing_fields\n",
    "    new_column_percentage = len(new_columns) / len(existing_fields)\n",
    "\n",
    "    if new_column_percentage > max_new_column_percentage:\n",
    "        error_msg = (\n",
    "            \"\\n\" + \"=\"*80 + \"\\n\" +\n",
    "            \"SCHEMA VALIDATION ERROR\\n\" +\n",
    "            \"=\"*80 + \"\\n\" +\n",
    "            \"The incoming DataFrame schema contains too many new columns.\\n\\n\" +\n",
    "            f\"Existing table column count: {len(existing_fields)}\\n\" +\n",
    "            f\"New columns detected ({len(new_columns)}):\\n\" +\n",
    "            \"\\n\".join(f\"  - {col}\" for col in sorted(new_columns)) + \"\\n\\n\" +\n",
    "            f\"New column percentage: {new_column_percentage:.2%}\\n\" +\n",
    "            f\"Maximum allowed: {max_new_column_percentage:.2%}\\n\" +\n",
    "            \"=\"*80\n",
    "        )\n",
    "        logger.error(error_msg)\n",
    "        return False, error_msg\n",
    "\n",
    "    return True, \"Schema changes within acceptable limits.\"\n",
    "\n",
    "def table_exists(table: str) -> bool:\n",
    "    try:\n",
    "        spark.table(table).limit(1).collect()\n",
    "        logger.info(f\"Table exists: {table}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.info(f\"Table does not exist: {table}. Exception: {e}\")\n",
    "        return False\n",
    "\n",
    "def adjust_schema(incoming_df, base_schema):\n",
    "    for field in base_schema.fields:\n",
    "        if field.name not in incoming_df.columns:\n",
    "            logger.info(f\"Adding missing column '{field.name}' with null values.\")\n",
    "            incoming_df = incoming_df.withColumn(field.name, F.lit(None).cast(field.dataType))\n",
    "    return incoming_df\n",
    "\n",
    "try:\n",
    "    if not table_exists(full_table_name):\n",
    "        logger.info(\"Delta table does not exist. Creating a new managed Delta table...\")\n",
    "        df_clean = df.select([F.col(c).alias(c.replace(' ', '_')) for c in df.columns])\n",
    "        df_clean.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "        logger.info(f\"DataFrame successfully written to: {full_table_name}\")\n",
    "    else:\n",
    "        logger.info(\"Delta table exists. Validating schema changes...\")\n",
    "        existing_schema = spark.table(full_table_name).schema\n",
    "\n",
    "        validation_result = validate_schema_changes(\n",
    "            existing_schema,\n",
    "            df.schema,\n",
    "            max_new_column_percentage=0.4,\n",
    "            min_schema_match_percentage=0.8\n",
    "        )\n",
    "\n",
    "        metrics = get_schema_metrics(\n",
    "            existing_schema,\n",
    "            df.schema,\n",
    "            max_new_column_percentage=0.4,\n",
    "            min_schema_match_percentage=0.8,\n",
    "            validation_result=validation_result\n",
    "        )\n",
    "        logger.info(f\"Schema evolution metrics: {metrics}\")\n",
    "\n",
    "        if not validation_result[0]:\n",
    "            raise ValueError(\"Schema validation error encountered. Please review the log above for details.\")\n",
    "\n",
    "        logger.info(\"Schema validation passed. Adjusting DataFrame schema if necessary...\")\n",
    "        df_adjusted = adjust_schema(df, existing_schema)\n",
    "\n",
    "        pre_append_count = spark.table(full_table_name).count()\n",
    "        logger.info(f\"Record count before append: {pre_append_count}\")\n",
    "\n",
    "        df_adjusted.write.format(\"delta\").mode(\"append\").saveAsTable(full_table_name)\n",
    "        logger.info(\"Append operation successful.\")\n",
    "\n",
    "        post_append_count = spark.table(full_table_name).count()\n",
    "        logger.info(f\"Record count after append: {post_append_count}\")\n",
    "        logger.info(f\"Records added: {post_append_count - pre_append_count}\")\n",
    "\n",
    "    logger.info(\"Previewing the first 5 rows from the Delta table:\")\n",
    "    display(spark.table(full_table_name).limit(5))\n",
    "\n",
    "except ValueError as ve:\n",
    "    logger.error(str(ve))\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"An unexpected error occurred during Delta table processing.\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "General Ingestion Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
